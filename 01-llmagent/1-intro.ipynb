{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ba230c",
   "metadata": {},
   "source": [
    "# Easy Agent Tutorial\n",
    "This notebook file provide three examples of using LLM based agents with different tool sets.\n",
    "\n",
    "prequisites:\n",
    "- Python 3.10+\n",
    "- Install required packages:\n",
    "  ```bash\n",
    "  pip install \"mcp[cli]\" smolagents\n",
    "  ```\n",
    "\n",
    "## Task Description\n",
    "\n",
    "如README所述，该项目应用三种方案，从不同的角度实现了agentic RAG的功能。为了演示，这一次我们将会构建一个信安四大会的查询，来进行感兴趣论文的搜索以及基于题目选择合适的会议进行投稿。\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a91a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们将使用dblp数据集来进行演示。首先下载四大会最近几年的会议论文数据：\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"dblp_sec_papers.json\"):\n",
    "    with requests.Session() as sess:\n",
    "        url = \"https://dblp.org/search/publ/api\"\n",
    "        params = {\"q\": \"security\", \"format\": \"json\"}\n",
    "        tocs = {\n",
    "            \"ndss\": \"toc:db/conf/ndss/ndss2025.bht:\",\n",
    "            \"sp\": \"toc:db/conf/sp/sp2025.bht:\",\n",
    "            \"ccs\": \"toc:db/conf/ccs/ccs2025.bht:\",\n",
    "            \"usenix\": \"toc:db/conf/uss/uss2025.bht:\",\n",
    "        }\n",
    "        papers = []\n",
    "        for k, v in tocs.items():\n",
    "            response = sess.get(url, params={\"q\": v, \"h\": 1000, \"format\": \"json\"})\n",
    "            data = response.json()\n",
    "            data = data[\"result\"][\"hits\"][\"hit\"]\n",
    "            papers.extend(data)\n",
    "        with open(f\"dblp_sec_papers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    with open(f\"dblp_sec_papers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        papers = json.load(f)\n",
    "papers = [x[\"info\"] for x in papers]\n",
    "titles = [f\"[{x['venue']} {x['year']}] {x['title']}\" for x in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff07b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建并保存向量数据库\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"db\" not in globals():\n",
    "    db = FAISS.from_texts(titles, OpenAIEmbeddings())\n",
    "db.save_local(\"faiss_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352eb4d",
   "metadata": {},
   "source": [
    "## 方案0： 一切奇迹的始发点——传统工具调用\n",
    "\n",
    "在开始之前，先看一下传统的工具调用大概长啥样，有怎样的优缺点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ba0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"kw\":\"blockchain LLM papers\"}', call_id='call_ZNk69x2z6uSxuPEyhldVmhRc', name='query_db', type='function_call', id='fc_0edcc91604d6e0a4006938e28cf5c0819282e662d761ad49ea', status='completed')]\n",
      "Keyword: blockchain LLM papers\n",
      "Searching for keyword: blockchain\n",
      "Searching for keyword: LLM\n",
      "Searching for keyword: papers\n",
      "kw='blockchain LLM papers' appended.\n",
      "你这个方向目前还比较前沿，严格意义上“区块链 × LLM”的论文不算多，但已经有几类比较典型的结合方式，可以按“区块链为LLM赋能”和“LLM为区块链赋能”来找文献。我先列代表性论文和关键词，方便你自己去搜（Google Scholar / arXiv / dblp），再给你一个按方向分类的阅读建议。\n",
      "\n",
      "下面所有英文标题你直接复制去搜索就能找到 PDF。\n",
      "\n",
      "---\n",
      "\n",
      "## 一、LLM 为区块链赋能（用 LLM 做智能合约/链上安全/分析）\n",
      "\n",
      "### 1. 智能合约分析与形式化验证\n",
      "\n",
      "- **PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation**  \n",
      "  NDSS 2025（网络与分布式系统安全研讨会）  \n",
      "  关键词：  \n",
      "  - “LLM-driven Formal Verification of Smart Contracts”  \n",
      "  - “Retrieval-Augmented Property Generation”  \n",
      "  核心思路：用 LLM 自动生成合约的安全属性 / 规范，再结合形式化验证工具检查。适合关注“LLM + 智能合约安全”的同学。\n",
      "\n",
      "- 可一并检索的关键词：  \n",
      "  - “LLM for smart contract auditing”  \n",
      "  - “LLM-based smart contract vulnerability detection”  \n",
      "  - “GPT-4 for smart contract analysis”  \n",
      "\n",
      "很多不是顶会论文，而是 arXiv / 工程论文，但实用性强。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 区块链安全、交易/地址分析\n",
      "\n",
      "- **Towards Explainable and Effective Anti-Money Laundering for Cryptocurrency**  \n",
      "  CCS 2025（计算机与通信安全大会）  \n",
      "  虽然标题没直接写 LLM，但工作里会用到深度学习/自然语言特征做可解释反洗钱；你可以配合以下关键词找更直接用 LLM 的：\n",
      "\n",
      "  推荐额外检索：\n",
      "  - “LLM for cryptocurrency transaction analysis”  \n",
      "  - “LLM-based blockchain address classification”  \n",
      "  - “LLM for DeFi risk analysis”\n",
      "\n",
      "- **Ghost Clusters: Evaluating Attribution of Illicit Services through Cryptocurrency Tracing**  \n",
      "  USENIX Security 2025  \n",
      "  主体是链上追踪和聚类，但你可以把它当成数据集/方法基础，再看后续有没有工作用 LLM 做实体归属、标签生成。\n",
      "\n",
      "---\n",
      "\n",
      "### 3. LLM 辅助 Web3/合约开发生命周期\n",
      "\n",
      "这些大多是 2023–2025 的 arXiv/Workshop 论文，可以用关键词搜：\n",
      "\n",
      "- “RAG for smart contract development”  \n",
      "- “LLM-powered Web3 development assistant”  \n",
      "- “LLM-based DeFi protocol analysis”\n",
      "\n",
      "可以重点找几个典型的：\n",
      "- “Using ChatGPT/GPT-4 for smart contract development: empirical study”\n",
      "- “LLM-based assistant for Solidity debugging”\n",
      "\n",
      "---\n",
      "\n",
      "## 二、区块链为 LLM 赋能（用区块链做 LLM 的激励/溯源/对齐）\n",
      "\n",
      "目前主流会议里“真正把 LLM 和链耦合在协议里”的工作还比较少，多数在 arXiv / 行业白皮书，典型方向包括：\n",
      "\n",
      "### 1. 模型溯源、版权与水印登记\n",
      "\n",
      "关键词可以这样搜：\n",
      "\n",
      "- “blockchain-based provenance for LLM outputs”  \n",
      "- “on-chain provenance for AI-generated content”  \n",
      "- “blockchain watermarking LLM text”  \n",
      "\n",
      "可以和这类论文一起看（虽然不一定都上链，但概念相近）：\n",
      "- **Provably Robust Multi-bit Watermarking for AI-generated Text**  \n",
      "  NDSS 2025  \n",
      "  再结合 “blockchain for robust watermark registry” 会有一些后续工作。\n",
      "\n",
      "### 2. 分布式/去中心化 LLM 训练与推理激励\n",
      "\n",
      "典型关键词（很多是 arXiv / 项目白皮书）：\n",
      "- “decentralized training of LLM with blockchain incentives”  \n",
      "- “token-incentivized LLM inference marketplace”  \n",
      "- “blockchain-based federated learning for LLM”  \n",
      "\n",
      "可以找类似：\n",
      "- “Bittensor: A Peer-to-Peer Market for Machine Intelligence”（项目白皮书/论文，典型“区块链 + 模型市场”）\n",
      "- “Gensyn” / “Akash AI” 等会有分布式算力+AI 的论文/白皮书\n",
      "\n",
      "---\n",
      "\n",
      "## 三、“安全视角”下的区块链 × LLM（安全圈目前最活跃）\n",
      "\n",
      "这些不是“在协议层面紧密耦合”的区块链+LLM，但对你理解安全问题很有帮助：\n",
      "\n",
      "### 1. LLM 帮你搞区块链安全\n",
      "\n",
      "- **YuraScanner: Leveraging LLMs for Task-driven Web App Scanning**  \n",
      "  NDSS 2025  \n",
      "  侧重点是 Web 安全扫描，但方法可以迁移到 Web3 前端 / 钱包 DApp 安全扫描。\n",
      "\n",
      "- “LLM-based fuzzing for smart contracts”  \n",
      "  可以和下面这篇一起看思路：  \n",
      "  - **Hybrid Language Processor Fuzzing via LLM-Based Constraint Solving**  \n",
      "    USENIX Security 2025  \n",
      "\n",
      "### 2. 区块链场景中的 LLM 风险（间接交叉）\n",
      "\n",
      "当你用 LLM 去构建 Web3 应用时，这类论文里讲的攻击/防御很关键：\n",
      "\n",
      "- **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  \n",
      "  USENIX Security 2025  \n",
      "  适用于 “有钱包 / 区块链读写能力的 Agent” 场景安全分析。\n",
      "\n",
      "- **On the (In)Security of LLM App Stores** – S&P 2025  \n",
      "  如果你构建“插件化的 Web3 Agent 商店”，这里的威胁模型可以直接借用。\n",
      "\n",
      "- 一些对 LLM 越狱、安全性、隐私的论文：  \n",
      "  - SelfDefend, PAPILLON, Activation Approximations…, PrivacyXray, Private Investigator, TracLLM 等（USENIX / NDSS / S&P 2025）  \n",
      "  在设计“链上 Agent DAO”“自治交易机器人”时，LLM 的安全问题会直接映射成资金风险。\n",
      "\n",
      "---\n",
      "\n",
      "## 四、给你一个检索策略（方便你扩展）\n",
      "\n",
      "你可以这样在 Google Scholar / arXiv 搜：\n",
      "\n",
      "1. 直接搜组合关键词：\n",
      "   - `\"blockchain\" \"large language model\"`  \n",
      "   - `\"web3\" \"large language model\"`  \n",
      "   - `\"smart contract\" \"large language model\"`  \n",
      "   - `\"DeFi\" \"large language model\"`  \n",
      "\n",
      "2. 指定近两年时间：\n",
      "   - Google Scholar 左侧 “Since 2023 / 2024 / 2025”\n",
      "\n",
      "3. 结合你关心的子方向：\n",
      "   - 安全：`\"LLM\" \"smart contract security\"` / `\"LLM\" \"DeFi risk\"`  \n",
      "   - 激励与协议：`\"LLM marketplace\" \"blockchain\"`  \n",
      "   - 隐私与合规：`\"on-chain provenance\" \"LLM\"` / `\"GDPR\" \"LLM\" \"blockchain\"`\n",
      "\n",
      "---\n",
      "\n",
      "## 五、如果你告诉我更细的兴趣，我可以给你“精读清单”\n",
      "\n",
      "比如你更偏向：\n",
      "\n",
      "- 做“LLM 辅助智能合约审计/验证”  \n",
      "- 做“去中心化 LLM / 模型市场 / Agent DAO 协议设计”  \n",
      "- 或者“用区块链做 LLM 输出的溯源、版权和监管”\n",
      "\n",
      "你说一下具体方向，我可以帮你整理一个更窄的 5–10 篇“必读 + 可选”列表，并按阅读顺序、重点 section 给你标出来。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import dotenv\n",
    "import tenacity\n",
    "import yaml\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "from openai.types import *\n",
    "from openai.types.chat import *\n",
    "\n",
    "\n",
    "def raw_toolcall():\n",
    "    client = OpenAI()\n",
    "    db = FAISS.load_local(\n",
    "        \"faiss_db\",\n",
    "        OpenAIEmbeddings(),\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    tools_def = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"query_db\",\n",
    "            \"description\": \"accept keyword and return related information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"kw\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The keyword to query the database\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"kw\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def real_ask(question: str):\n",
    "        from openai.types.responses.response_input_param import Message\n",
    "\n",
    "        input_msgs: list[Message] = [\n",
    "            {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful research assistant. When given a question, you must first decide if you need to query the database to get relevant information. If so, use the tool 'query_db' with appropriate keywords extracted from the question. After getting the information, provide a comprehensive answer based on both the retrieved information and your own knowledge.\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ]\n",
    "        resp = client.responses.create(\n",
    "            model=\"gpt-5.1\",\n",
    "            tools=tools_def,\n",
    "            input=input_msgs,\n",
    "            tool_choice=\"required\",\n",
    "        )\n",
    "        print(resp.output)\n",
    "        for toolcall in resp.output:\n",
    "            if toolcall.type != \"function_call\":\n",
    "                continue\n",
    "            if toolcall.name == \"query_db\":\n",
    "                kw = json.loads(toolcall.arguments)[\"kw\"]\n",
    "                print(f\"Keyword: {kw}\")\n",
    "                docs = []\n",
    "                for skw in kw.split():\n",
    "                    if not (skw := skw.strip()):\n",
    "                        continue\n",
    "                    print(f\"Searching for keyword: {skw}\")\n",
    "                    docs.extend(db.similarity_search(skw, k=30))\n",
    "                rag_result = \"\\n\".join([doc.page_content for doc in docs])\n",
    "                input_msgs.append(toolcall)\n",
    "                input_msgs.append(\n",
    "                    {\n",
    "                        \"type\": \"function_call_output\",\n",
    "                        \"call_id\": toolcall.call_id,\n",
    "                        \"output\": str(rag_result),\n",
    "                    }\n",
    "                )\n",
    "                print(f\"{kw=} appended.\")\n",
    "        if input_msgs[-1][\"type\"] == \"function_call_output\":\n",
    "            resp = client.responses.create(\n",
    "                model=\"gpt-5.1\",\n",
    "                input=input_msgs,\n",
    "                # tools=tools_def,\n",
    "                stream=True,\n",
    "            )\n",
    "            for chunk in resp:\n",
    "                if chunk.type == \"response.output_text.delta\":\n",
    "                    print(chunk.delta, end=\"\", flush=True)\n",
    "            print()\n",
    "        else:\n",
    "            print(resp.output_text)\n",
    "\n",
    "    while True:\n",
    "        inp = input(\"=> \")\n",
    "        if not inp.strip():\n",
    "            break\n",
    "        real_ask(inp)\n",
    "\n",
    "\n",
    "raw_toolcall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0f2b1",
   "metadata": {},
   "source": [
    "### 传统工具调用的优缺点\n",
    "\n",
    "- 优点：直接用模型原生的 function/tool 调用协议，链路短、开销低，JSON Schema 参数校验清晰。\n",
    "- 优点：可以精确控制何时调用工具、使用 `tool_choice` 等参数强制执行，消息格式透明、便于调试和流式输出。\n",
    "- 优点：依赖少，不绑框架，易于插入到现有服务或与其他编排层组合。\n",
    "- 缺点：需要手写对话状态管理、工具输入输出拼接，容易出错且样板代码多。\n",
    "- 缺点：缺少自动规划/多步推理、重试、fallback 等封装能力，复杂流程要自行实现。\n",
    "- 缺点：与特定模型/协议耦合，换提供方或多模型时需适配；安全性与数据清洗（如反序列化、去重）也要自管。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888af19",
   "metadata": {},
   "source": [
    "## SmolAgent::CodeAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d330b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">You are a helpful research assistant.</span>                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">When given a question, you must query the database to get relevant information.</span>                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Use the tools with appropriate arguments derived from the question.</span>                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">After getting the information, provide a comprehensive answer based on both the retrieved information.</span>          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Output the final answer in markdown wrapped in final_answer().</span>                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">查询llm安全相关的论文</span>                                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIModel - gpt-5.1 ─────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYou are a helpful research assistant.\u001b[0m                                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhen given a question, you must query the database to get relevant information.\u001b[0m                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUse the tools with appropriate arguments derived from the question.\u001b[0m                                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mAfter getting the information, provide a comprehensive answer based on both the retrieved information.\u001b[0m          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mOutput the final answer in markdown wrapped in final_answer().\u001b[0m                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1m查询llm安全相关的论文\u001b[0m                                                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIModel - gpt-5.1 \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0fc5d2791e43fe850a9a2053eabf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> typing </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> List</span><span style=\"background-color: #272822\">                                                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Step 1: Query the paper database for LLM safety related keywords (in English and Chinese)</span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLM safety|large language model safety|AI alignment|红队 攻击 LLM|大模型 安全 防护|jailbreak defense\"</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">papers_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> query_paperdb(kw</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw)</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(papers_info)</span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtyping\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mList\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Step 1: Query the paper database for LLM safety related keywords (in English and Chinese)\u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLM safety|large language model safety|AI alignment|红队 攻击 LLM|大模型 安全 防护|jailbreak defense\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpapers_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery_paperdb\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpapers_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed at line 'from typing import List' due to: InterpreterError: Import from typing is not </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">allowed. Authorized imports are: ['queue', 'collections', 'unicodedata', 'datetime', 'stat', 'random', 're', </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'math', 'statistics', 'time', 'itertools'\\]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode execution failed at line 'from typing import List' due to: InterpreterError: Import from typing is not \u001b[0m\n",
       "\u001b[1;31mallowed. Authorized imports are: ['queue', 'collections', 'unicodedata', 'datetime', 'stat', 'random', 're', \u001b[0m\n",
       "\u001b[1;31m'math', 'statistics', 'time', 'itertools'\\]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.12 seconds| Input tokens: 2,096 | Output tokens: 94]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.12 seconds| Input tokens: 2,096 | Output tokens: 94]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0afca150b4b4fe4bf7ffec51b1ea058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 第一步：用中英文关键词查询与 LLM 安全相关的论文</span><span style=\"background-color: #272822\">                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLM safety|large language model safety|foundation model safety|jailbreak|red teaming|prompt </span><span style=\"background-color: #272822\">             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">injection|alignment|对齐 安全|大语言模型 安全|大模型 安全 攻击 防御|越狱 攻击 LLM\"</span><span style=\"background-color: #272822\">                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">papers_raw </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> query_paperdb(kw</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw)</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(papers_raw)</span><span style=\"background-color: #272822\">                                                                                              </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# 第一步：用中英文关键词查询与 LLM 安全相关的论文\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLM safety|large language model safety|foundation model safety|jailbreak|red teaming|prompt \u001b[0m\u001b[48;2;39;40;34m             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34minjection|alignment|对齐 安全|大语言模型 安全|大模型 安全 攻击 防御|越狱 攻击 LLM\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpapers_raw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery_paperdb\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpapers_raw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for keyword: LLM safety\n",
      "Searching for keyword: large language model safety\n",
      "Searching for keyword: foundation model safety\n",
      "Searching for keyword: jailbreak\n",
      "Searching for keyword: red teaming\n",
      "Searching for keyword: prompt injection\n",
      "Searching for keyword: alignment\n",
      "Searching for keyword: 对齐 安全\n",
      "Searching for keyword: 大语言模型 安全\n",
      "Searching for keyword: 大模型 安全 攻击 防御\n",
      "Searching for keyword: 越狱 攻击 LLM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "[USENIX Security Symposium 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain.\n",
       "[SP 2025] On the (In)Security of LLM App Stores.\n",
       "[USENIX Security Symposium 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: \n",
       "Comprehensive Analysis and Defense.\n",
       "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in \n",
       "AI Web Search.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&amp;apos; \n",
       "Refusal Boundaries.\n",
       "[USENIX Security Symposium 2025] EchoLLM: LLM-Augmented Acoustic Eavesdropping Attack on Bone Conduction Headphones\n",
       "with mmWave Radar.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language \n",
       "Models.\n",
       "[USENIX Security Symposium 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language \n",
       "Models on Generated Data.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] From Purity to Peril: Backdooring Merged Models From &amp;quot;Harmless&amp;quot; Benign \n",
       "Components.\n",
       "[USENIX Security Symposium 2025] Lancet: A Formalization Framework for Crash and Exploit Pathology.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models \n",
       "with Limited Data.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] SoK: A Framework and Guide for Human-Centered Threat Modeling in Security and Privacy Research.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image \n",
       "Generation Models.\n",
       "[USENIX Security Symposium 2025] ChoiceJacking: Compromising Mobile Devices through Malicious Chargers like a \n",
       "Decade ago.\n",
       "[USENIX Security Symposium 2025] TapTrap: Animation-Driven Tapjacking on Android.\n",
       "[USENIX Security Symposium 2025] Red Bleed: A Pragmatic Near-Infrared Presentation Attack on Facial Biometric \n",
       "Authentication Systems.\n",
       "[USENIX Security Symposium 2025] &amp;quot;Threat modeling is very formal, it&amp;apos;s very technical, and also very hard\n",
       "to do correctly&amp;quot;: Investigating Threat Modeling Practices in Open-Source Software Projects.\n",
       "[USENIX Security Symposium 2025] Revisiting Training-Inference Trigger Intensity in Backdoor Attacks.\n",
       "[USENIX Security Symposium 2025] Beyond Exploit Scanning: A Functional Change-Driven Approach to Remote Software \n",
       "Version Identification.\n",
       "[USENIX Security Symposium 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for \n",
       "Static Analysis Result Verification.\n",
       "[USENIX Security Symposium 2025] Towards a Re-evaluation of Data Forging Attacks in Practice.\n",
       "[USENIX Security Symposium 2025] Principled and Automated Approach for Investigating AR/VR Attacks.\n",
       "[USENIX Security Symposium 2025] IDFuzz: Intelligent Directed Grey-box Fuzzing.\n",
       "[USENIX Security Symposium 2025] High Stakes, Low Certainty: Evaluating the Efficacy of High-Level Indicators of \n",
       "Compromise in Ransomware Attribution.\n",
       "[USENIX Security Symposium 2025] Cyber-Physical Deception Through Coordinated IoT Honeypots.\n",
       "[USENIX Security Symposium 2025] StruQ: Defending Against Prompt Injection with Structured Queries.\n",
       "[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.\n",
       "[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models.\n",
       "[USENIX Security Symposium 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services.\n",
       "[USENIX Security Symposium 2025] Private Investigator: Extracting Personally Identifiable Information from Large \n",
       "Language Models Using Optimized Prompts.\n",
       "[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection \n",
       "Attacks via the Fine-Tuning Interface.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[CCS 2025] PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and \n",
       "Verification.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&amp;apos; \n",
       "Refusal Boundaries.\n",
       "[SP 2025] Characterizing Robocalls with Multiple Vantage Points.\n",
       "[CCS 2025] Elastic Restaking Networks: United we fall, (partially) divided we stand.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] Connecting the Extra Dots (Contexts): Correlating External Information about Point of Interest for Attack\n",
       "Investigation.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] Flexway O-Sort: Enclave-Friendly and Optimal Oblivious Sorting.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Efficient 2PC for Constant Round Secure Equality Testing and Comparison.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] Found in Translation: A Generative Language Modeling Approach to Memory Access \n",
       "Pattern Attacks.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language \n",
       "Models.\n",
       "[CCS 2025] Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving.\n",
       "[USENIX Security Symposium 2025] Pretender: Universal Active Defense against Diffusion Finetuning Attacks.\n",
       "[CCS 2025] Poster: Black-box Attacks on Multimodal Large Language Models through Adversarial ICC Profiles.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[NDSS 2025] Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[SP 2025] Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom \n",
       "Attacks.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM \n",
       "Analysis.\n",
       "[NDSS 2025] LLMPirate: LLMs for Black-box Hardware IP Piracy.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[CCS 2025] MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "[USENIX Security Symposium 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain.\n",
       "[SP 2025] On the (In)Security of LLM App Stores.\n",
       "[USENIX Security Symposium 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: \n",
       "Comprehensive Analysis and Defense.\n",
       "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in \n",
       "AI Web Search.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&apos; \n",
       "Refusal Boundaries.\n",
       "[USENIX Security Symposium 2025] EchoLLM: LLM-Augmented Acoustic Eavesdropping Attack on Bone Conduction Headphones\n",
       "with mmWave Radar.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language \n",
       "Models.\n",
       "[USENIX Security Symposium 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language \n",
       "Models on Generated Data.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] From Purity to Peril: Backdooring Merged Models From &quot;Harmless&quot; Benign \n",
       "Components.\n",
       "[USENIX Security Symposium 2025] Lancet: A Formalization Framework for Crash and Exploit Pathology.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models \n",
       "with Limited Data.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] SoK: A Framework and Guide for Human-Centered Threat Modeling in Security and Privacy Research.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image \n",
       "Generation Models.\n",
       "[USENIX Security Symposium 2025] ChoiceJacking: Compromising Mobile Devices through Malicious Chargers like a \n",
       "Decade ago.\n",
       "[USENIX Security Symposium 2025] TapTrap: Animation-Driven Tapjacking on Android.\n",
       "[USENIX Security Symposium 2025] Red Bleed: A Pragmatic Near-Infrared Presentation Attack on Facial Biometric \n",
       "Authentication Systems.\n",
       "[USENIX Security Symposium 2025] &quot;Threat modeling is very formal, it&apos;s very technical, and also very hard\n",
       "to do correctly&quot;: Investigating Threat Modeling Practices in Open-Source Software Projects.\n",
       "[USENIX Security Symposium 2025] Revisiting Training-Inference Trigger Intensity in Backdoor Attacks.\n",
       "[USENIX Security Symposium 2025] Beyond Exploit Scanning: A Functional Change-Driven Approach to Remote Software \n",
       "Version Identification.\n",
       "[USENIX Security Symposium 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for \n",
       "Static Analysis Result Verification.\n",
       "[USENIX Security Symposium 2025] Towards a Re-evaluation of Data Forging Attacks in Practice.\n",
       "[USENIX Security Symposium 2025] Principled and Automated Approach for Investigating AR/VR Attacks.\n",
       "[USENIX Security Symposium 2025] IDFuzz: Intelligent Directed Grey-box Fuzzing.\n",
       "[USENIX Security Symposium 2025] High Stakes, Low Certainty: Evaluating the Efficacy of High-Level Indicators of \n",
       "Compromise in Ransomware Attribution.\n",
       "[USENIX Security Symposium 2025] Cyber-Physical Deception Through Coordinated IoT Honeypots.\n",
       "[USENIX Security Symposium 2025] StruQ: Defending Against Prompt Injection with Structured Queries.\n",
       "[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.\n",
       "[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models.\n",
       "[USENIX Security Symposium 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services.\n",
       "[USENIX Security Symposium 2025] Private Investigator: Extracting Personally Identifiable Information from Large \n",
       "Language Models Using Optimized Prompts.\n",
       "[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection \n",
       "Attacks via the Fine-Tuning Interface.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[CCS 2025] PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and \n",
       "Verification.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&apos; \n",
       "Refusal Boundaries.\n",
       "[SP 2025] Characterizing Robocalls with Multiple Vantage Points.\n",
       "[CCS 2025] Elastic Restaking Networks: United we fall, (partially) divided we stand.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] Connecting the Extra Dots (Contexts): Correlating External Information about Point of Interest for Attack\n",
       "Investigation.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] Flexway O-Sort: Enclave-Friendly and Optimal Oblivious Sorting.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Efficient 2PC for Constant Round Secure Equality Testing and Comparison.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] Found in Translation: A Generative Language Modeling Approach to Memory Access \n",
       "Pattern Attacks.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language \n",
       "Models.\n",
       "[CCS 2025] Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving.\n",
       "[USENIX Security Symposium 2025] Pretender: Universal Active Defense against Diffusion Finetuning Attacks.\n",
       "[CCS 2025] Poster: Black-box Attacks on Multimodal Large Language Models through Adversarial ICC Profiles.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[NDSS 2025] Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[SP 2025] Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom \n",
       "Attacks.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM \n",
       "Analysis.\n",
       "[NDSS 2025] LLMPirate: LLMs for Black-box Hardware IP Piracy.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[CCS 2025] MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 8.75 seconds| Input tokens: 4,503 | Output tokens: 257]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 8.75 seconds| Input tokens: 4,503 | Output tokens: 257]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e8a4f64d624972afa3730b0f22e72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 这里我们已经拿到了大量与“大语言模型（LLM）安全”高度相关的论文题目</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 接下来不再调用工具，而是基于这些结果，按主题给你做一个中文综述与阅读建议。</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">summary </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">r\"\"\"</span><span style=\"background-color: #272822\">                                                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  </span><span style=\"background-color: #272822\">                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">（会议缩写：SP=IEEE S&amp;P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）</span><span style=\"background-color: #272822\">                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 一、总体图景：LLM 安全/对齐的主要研究方向</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">从检索结果看，LLM 安全论文大致可以分为几大类：</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **越狱（Jailbreak）与红队攻击**</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 同时研究防御策略，例如对齐增强、概念级控制等。</span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **提示注入（Prompt Injection）、Prompt 窃取和推断**</span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 提出结构化查询、对齐优化等防御机制。</span><span style=\"background-color: #272822\">                                                                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。</span><span style=\"background-color: #272822\">                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **对齐与“安全失配”（Safety Misalignment）**</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **隐私与数据泄露**</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 从模型中提取个人敏感信息（PII）。</span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 成员推断（membership inference）与训练数据保护。</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">5. **后门、模型合并与生成数据风险**</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 大模型后门攻击与检测方法。</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 模型合并（model merging）中的隐含安全风险。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">6. **面向特定群体/应用的安全**</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 面向青少年/未成年人内容安全。</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。</span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">7. **多模态安全（Vision-Language / Text-to-Image）**</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 图文大模型识别不安全内容的能力评估与改进。</span><span style=\"background-color: #272822\">                                                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 文本到图像模型的越狱与防御。</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。</span><span style=\"background-color: #272822\">                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 二、越狱攻击与防御（Jailbreak / Red-teaming）</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**攻击方法类：**</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  </span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。</span><span style=\"background-color: #272822\">             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  </span><span style=\"background-color: #272822\">                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。</span><span style=\"background-color: #272822\">                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Text-to-Image Generation Models**  </span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。</span><span style=\"background-color: #272822\">                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**防御方法类：**</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  </span><span style=\"background-color: #272822\">        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。</span><span style=\"background-color: #272822\">                                              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 实践性：强调可部署性，对工程系统启发大。</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Analysis and Manipulation**  </span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。</span><span style=\"background-color: #272822\">                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">for LLMs**  </span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。</span><span style=\"background-color: #272822\">                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 三、提示注入、提示窃取与结构化防御</span><span style=\"background-color: #272822\">                                                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**攻击与测量：**</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  </span><span style=\"background-color: #272822\">                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。</span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 有助于理解 Prompt 作为“知识资产”的威胁模型。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  </span><span style=\"background-color: #272822\">             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  </span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 场景：多方推理、分布式推理中的 prompt 泄露风险。</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Injection Attacks via the Fine-Tuning Interface**  </span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。</span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**防御与对齐优化：**</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  </span><span style=\"background-color: #272822\">                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。</span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  </span><span style=\"background-color: #272822\">                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。</span><span style=\"background-color: #272822\">                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。</span><span style=\"background-color: #272822\">                                                </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 四、安全对齐与“(In)Security / Misalignment”问题</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[NDSS 2025] Safety Misalignment Against Large Language Models**  </span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  </span><span style=\"background-color: #272822\">                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 关注：下游微调是否、以及如何破坏原有安全对齐。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提供：策略来在微调时尽量保持对齐属性。</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  </span><span style=\"background-color: #272822\">           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 帮助你理解对齐机制的脆弱点。</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。</span><span style=\"background-color: #272822\">                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Analysis and Defense**  </span><span style=\"background-color: #272822\">                                                                                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。</span><span style=\"background-color: #272822\">                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 五、隐私、成员推断与敏感数据安全</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Domain**  </span><span style=\"background-color: #272822\">                                                                                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 适合：对医疗+大模型交叉感兴趣的方向。</span><span style=\"background-color: #272822\">                                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Models Using Optimized Prompts**  </span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 展示：优化提示如何从看似安全的模型中挖出 PII。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  </span><span style=\"background-color: #272822\"> </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。</span><span style=\"background-color: #272822\">                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Generated Data**  </span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。</span><span style=\"background-color: #272822\">                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  </span><span style=\"background-color: #272822\">                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。</span><span style=\"background-color: #272822\">                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 六、后门、模型合并和生成数据风险</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 利用嵌入空间特性构造可迁移/跨触发的后门。</span><span style=\"background-color: #272822\">                                                                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components**  </span><span style=\"background-color: #272822\">        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。</span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  </span><span style=\"background-color: #272822\">                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 七、青少年/特定群体与应用场景安全</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Search**  </span><span style=\"background-color: #272822\">                                                                                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。</span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] On the (In)Security of LLM App Stores**  </span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 类似“插件商店 / LLM App Store”的生态安全问题。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  </span><span style=\"background-color: #272822\">                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 八、多模态与图像相关安全（T2I / VLM）</span><span style=\"background-color: #272822\">                                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Text-to-Image Generation Models**  </span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  </span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。</span><span style=\"background-color: #272822\">                </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 九、如何系统入门 LLM 安全研究？</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **宏观理解 LLM 安全问题空间**</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 阅读：  </span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  </span><span style=\"background-color: #272822\">                                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  </span><span style=\"background-color: #272822\">                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。</span><span style=\"background-color: #272822\">         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 实践：  </span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  </span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - 改写系统提示、添加规则模型/过滤器，观察防御效果。</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **扩展到隐私&amp;后门等更深层次问题**</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。</span><span style=\"background-color: #272822\">                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **关注“上线场景”的综合安全**</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。</span><span style=\"background-color: #272822\">                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 十、如果你想要一份“快速论文清单”</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：</span><span style=\"background-color: #272822\">                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  </span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  </span><span style=\"background-color: #272822\">                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  </span><span style=\"background-color: #272822\">                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **SelfDefend / JBShield**（了解防御思路）  </span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  </span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  </span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  </span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">8. **YouthSafe**（了解“面向特定人群”的安全基准构建）</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">如果你告诉我：  </span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 你更偏「攻击」还是「防御」；  </span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 更想做「理论/算法」还是「工程系统/落地」；  </span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  </span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。</span><span style=\"background-color: #272822\">                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"\"\"</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(summary)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# 这里我们已经拿到了大量与“大语言模型（LLM）安全”高度相关的论文题目\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# 接下来不再调用工具，而是基于这些结果，按主题给你做一个中文综述与阅读建议。\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mr\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  \u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m（会议缩写：SP=IEEE S&P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）\u001b[0m\u001b[48;2;39;40;34m                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 一、总体图景：LLM 安全/对齐的主要研究方向\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m从检索结果看，LLM 安全论文大致可以分为几大类：\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **越狱（Jailbreak）与红队攻击**\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 同时研究防御策略，例如对齐增强、概念级控制等。\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **提示注入（Prompt Injection）、Prompt 窃取和推断**\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 提出结构化查询、对齐优化等防御机制。\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **对齐与“安全失配”（Safety Misalignment）**\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **隐私与数据泄露**\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 从模型中提取个人敏感信息（PII）。\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 成员推断（membership inference）与训练数据保护。\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m5. **后门、模型合并与生成数据风险**\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 大模型后门攻击与检测方法。\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 模型合并（model merging）中的隐含安全风险。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m6. **面向特定群体/应用的安全**\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 面向青少年/未成年人内容安全。\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m7. **多模态安全（Vision-Language / Text-to-Image）**\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 图文大模型识别不安全内容的能力评估与改进。\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 文本到图像模型的越狱与防御。\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 二、越狱攻击与防御（Jailbreak / Red-teaming）\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**攻击方法类：**\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  \u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  \u001b[0m\u001b[48;2;39;40;34m                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mText-to-Image Generation Models**  \u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**防御方法类：**\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 实践性：强调可部署性，对工程系统启发大。\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mAnalysis and Manipulation**  \u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mfor LLMs**  \u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。\u001b[0m\u001b[48;2;39;40;34m                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 三、提示注入、提示窃取与结构化防御\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**攻击与测量：**\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  \u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。\u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 有助于理解 Prompt 作为“知识资产”的威胁模型。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  \u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 场景：多方推理、分布式推理中的 prompt 泄露风险。\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mInjection Attacks via the Fine-Tuning Interface**  \u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。\u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**防御与对齐优化：**\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  \u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。\u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  \u001b[0m\u001b[48;2;39;40;34m                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。\u001b[0m\u001b[48;2;39;40;34m                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 四、安全对齐与“(In)Security / Misalignment”问题\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[NDSS 2025] Safety Misalignment Against Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  \u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 关注：下游微调是否、以及如何破坏原有安全对齐。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提供：策略来在微调时尽量保持对齐属性。\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 帮助你理解对齐机制的脆弱点。\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m Refusal Boundaries**  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mAnalysis and Defense**  \u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 五、隐私、成员推断与敏感数据安全\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mDomain**  \u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 适合：对医疗+大模型交叉感兴趣的方向。\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mModels Using Optimized Prompts**  \u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 展示：优化提示如何从看似安全的模型中挖出 PII。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mGenerated Data**  \u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  \u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 六、后门、模型合并和生成数据风险\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 利用嵌入空间特性构造可迁移/跨触发的后门。\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mHarmless\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m Benign Components**  \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。\u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  \u001b[0m\u001b[48;2;39;40;34m                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 七、青少年/特定群体与应用场景安全\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mSearch**  \u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] On the (In)Security of LLM App Stores**  \u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 类似“插件商店 / LLM App Store”的生态安全问题。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  \u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 八、多模态与图像相关安全（T2I / VLM）\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mText-to-Image Generation Models**  \u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  \u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 九、如何系统入门 LLM 安全研究？\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **宏观理解 LLM 安全问题空间**\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 阅读：  \u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  \u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  \u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。\u001b[0m\u001b[48;2;39;40;34m         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 实践：  \u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  \u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - 改写系统提示、添加规则模型/过滤器，观察防御效果。\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **扩展到隐私&后门等更深层次问题**\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **关注“上线场景”的综合安全**\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。\u001b[0m\u001b[48;2;39;40;34m                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 十、如果你想要一份“快速论文清单”\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  \u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  \u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  \u001b[0m\u001b[48;2;39;40;34m                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **SelfDefend / JBShield**（了解防御思路）  \u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  \u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  \u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  \u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m8. **YouthSafe**（了解“面向特定人群”的安全基准构建）\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m如果你告诉我：  \u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 你更偏「攻击」还是「防御」；  \u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 更想做「理论/算法」还是「工程系统/落地」；  \u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  \u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">（会议缩写：SP=IEEE S&amp;P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 一、总体图景：LLM 安全/对齐的主要研究方向</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">从检索结果看，LLM 安全论文大致可以分为几大类：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **越狱（Jailbreak）与红队攻击**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 同时研究防御策略，例如对齐增强、概念级控制等。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **提示注入（Prompt Injection）、Prompt 窃取和推断**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 提出结构化查询、对齐优化等防御机制。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **对齐与“安全失配”（Safety Misalignment）**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **隐私与数据泄露**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 从模型中提取个人敏感信息（PII）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 成员推断（membership inference）与训练数据保护。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">5. **后门、模型合并与生成数据风险**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 大模型后门攻击与检测方法。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 模型合并（model merging）中的隐含安全风险。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">6. **面向特定群体/应用的安全**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 面向青少年/未成年人内容安全。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">7. **多模态安全（Vision-Language / Text-to-Image）**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 图文大模型识别不安全内容的能力评估与改进。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 文本到图像模型的越狱与防御。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 二、越狱攻击与防御（Jailbreak / Red-teaming）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**攻击方法类：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Text-to-Image Generation Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**防御方法类：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 实践性：强调可部署性，对工程系统启发大。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Analysis and Manipulation**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">LLMs**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 三、提示注入、提示窃取与结构化防御</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**攻击与测量：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 有助于理解 Prompt 作为“知识资产”的威胁模型。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 场景：多方推理、分布式推理中的 prompt 泄露风险。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Injection Attacks via the Fine-Tuning Interface**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**防御与对齐优化：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 四、安全对齐与“(In)Security / Misalignment”问题</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[NDSS 2025] Safety Misalignment Against Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 关注：下游微调是否、以及如何破坏原有安全对齐。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提供：策略来在微调时尽量保持对齐属性。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 帮助你理解对齐机制的脆弱点。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Analysis and Defense**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 五、隐私、成员推断与敏感数据安全</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Domain**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 适合：对医疗+大模型交叉感兴趣的方向。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language Models </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Using Optimized Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 展示：优化提示如何从看似安全的模型中挖出 PII。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Generated Data**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 六、后门、模型合并和生成数据风险</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 利用嵌入空间特性构造可迁移/跨触发的后门。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 七、青少年/特定群体与应用场景安全</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search** </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] On the (In)Security of LLM App Stores**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 类似“插件商店 / LLM App Store”的生态安全问题。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 八、多模态与图像相关安全（T2I / VLM）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Text-to-Image Generation Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 九、如何系统入门 LLM 安全研究？</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **宏观理解 LLM 安全问题空间**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 阅读：  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 实践：  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - 改写系统提示、添加规则模型/过滤器，观察防御效果。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **扩展到隐私&amp;后门等更深层次问题**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **关注“上线场景”的综合安全**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 十、如果你想要一份“快速论文清单”</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **SelfDefend / JBShield**（了解防御思路）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">8. **YouthSafe**（了解“面向特定人群”的安全基准构建）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">如果你告诉我：  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 你更偏「攻击」还是「防御」；  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 更想做「理论/算法」还是「工程系统/落地」；  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m（会议缩写：SP=IEEE S&P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 一、总体图景：LLM 安全/对齐的主要研究方向\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m从检索结果看，LLM 安全论文大致可以分为几大类：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **越狱（Jailbreak）与红队攻击**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 同时研究防御策略，例如对齐增强、概念级控制等。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m2. **提示注入（Prompt Injection）、Prompt 窃取和推断**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 提出结构化查询、对齐优化等防御机制。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m3. **对齐与“安全失配”（Safety Misalignment）**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m4. **隐私与数据泄露**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 从模型中提取个人敏感信息（PII）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 成员推断（membership inference）与训练数据保护。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m5. **后门、模型合并与生成数据风险**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 大模型后门攻击与检测方法。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 模型合并（model merging）中的隐含安全风险。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m6. **面向特定群体/应用的安全**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 面向青少年/未成年人内容安全。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m7. **多模态安全（Vision-Language / Text-to-Image）**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 图文大模型识别不安全内容的能力评估与改进。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 文本到图像模型的越狱与防御。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 二、越狱攻击与防御（Jailbreak / Red-teaming）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**攻击方法类：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mText-to-Image Generation Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**防御方法类：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 实践性：强调可部署性，对工程系统启发大。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mAnalysis and Manipulation**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mLLMs**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 三、提示注入、提示窃取与结构化防御\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**攻击与测量：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 有助于理解 Prompt 作为“知识资产”的威胁模型。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 场景：多方推理、分布式推理中的 prompt 泄露风险。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mInjection Attacks via the Fine-Tuning Interface**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**防御与对齐优化：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 四、安全对齐与“(In)Security / Misalignment”问题\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[NDSS 2025] Safety Misalignment Against Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 关注：下游微调是否、以及如何破坏原有安全对齐。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提供：策略来在微调时尽量保持对齐属性。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 帮助你理解对齐机制的脆弱点。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mAnalysis and Defense**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 五、隐私、成员推断与敏感数据安全\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mDomain**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 适合：对医疗+大模型交叉感兴趣的方向。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language Models \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mUsing Optimized Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 展示：优化提示如何从看似安全的模型中挖出 PII。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mGenerated Data**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 六、后门、模型合并和生成数据风险\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 利用嵌入空间特性构造可迁移/跨触发的后门。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 七、青少年/特定群体与应用场景安全\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search** \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] On the (In)Security of LLM App Stores**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 类似“插件商店 / LLM App Store”的生态安全问题。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 八、多模态与图像相关安全（T2I / VLM）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mText-to-Image Generation Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 九、如何系统入门 LLM 安全研究？\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **宏观理解 LLM 安全问题空间**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 阅读：  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 实践：  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - 改写系统提示、添加规则模型/过滤器，观察防御效果。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m3. **扩展到隐私&后门等更深层次问题**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m4. **关注“上线场景”的综合安全**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 十、如果你想要一份“快速论文清单”\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m4. **SelfDefend / JBShield**（了解防御思路）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m8. **YouthSafe**（了解“面向特定人群”的安全基准构建）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m如果你告诉我：  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 你更偏「攻击」还是「防御」；  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 更想做「理论/算法」还是「工程系统/落地」；  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 53.20 seconds| Input tokens: 10,080 | Output tokens: 4,046]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 53.20 seconds| Input tokens: 10,080 | Output tokens: 4,046]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.token_usage=TokenUsage(input_tokens=10080, output_tokens=4046, total_tokens=14126) res.timing=Timing(start_time=1765350604.0020978, end_time=1765350669.0835783, duration=65.08148050308228)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from smolagents import CodeAgent, OpenAIServerModel, tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"faiss_db\",\n",
    "    OpenAIEmbeddings(),\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a helpful research assistant.\n",
    "When given a question, you must query the database to get relevant information.\n",
    "Use the tools with appropriate arguments derived from the question.\n",
    "After getting the information, provide a comprehensive answer based on both the retrieved information.\n",
    "Output the final answer in markdown wrapped in final_answer().\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "@tool\n",
    "def query_paperdb(kw: str) -> str:\n",
    "    \"\"\"\n",
    "    Accept keywords and return related paper titles, multiple keywords should be separated by '|'.\n",
    "    This tool should be called before output any realworld-related information.\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword to query the database\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for skw in kw.split(\"|\"):\n",
    "        if not (skw := skw.strip()):\n",
    "            continue\n",
    "        print(f\"Searching for keyword: {skw}\")\n",
    "        docs.extend(db.similarity_search(skw, k=10))\n",
    "    rag_result = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return rag_result\n",
    "\n",
    "\n",
    "agent = CodeAgent(\n",
    "    model=OpenAIServerModel(\"gpt-5.1\"),\n",
    "    tools=[query_paperdb],\n",
    "    stream_outputs=True,\n",
    "    # use_structured_outputs_internally=True, # True for structured_code_agent.yaml, False for code_agent.yaml\n",
    ")\n",
    "\n",
    "# from Gradio_UI import GradioUI\n",
    "\n",
    "# GradioUI(agent).launch()\n",
    "\n",
    "res = agent.run(f\"{sys_prompt}\\n\\n{input(\"=> \")}\", return_full_result=True)\n",
    "print(f\"{res.token_usage=} {res.timing=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5978dc",
   "metadata": {},
   "source": [
    "### 方案说明：SmolAgent::CodeAgent（两种提示模板差异）\n",
    "- 核心循环：两条线路都遵循多步推理，但输出格式不同。\n",
    "  - 标准版（`code_agent.yaml`，`use_structured_outputs_internally=False`）：每步输出 Thought→Code→Observation，代码必须包裹在自定义 `{{code_block_opening_tag}}`/`{{code_block_closing_tag}}` 中；中间结果用 `print` 进入 Observation，最终用 `final_answer` 收束。\n",
    "  - 结构化版（`structured_code_agent.yaml`，`use_structured_outputs_internally=True`）：每步输出固定 JSON `{ \"thought\": \"...\", \"code\": \"...\" }`，默认不需要自定义 code_block 标签；解析稳定、便于日志/回放。\n",
    "- 规划 scaffold：两者都要求先做 facts survey + high-level plan（initial_plan / update_plan），但标准版在文本里更强调「分步打印、避免链式依赖」。\n",
    "- 工具调用与链式策略：\n",
    "  - 标准版明确区分“有 JSON schema 的工具可以链式调用，非结构化工具应先 `print` 再下一步用”，并警示不要重做相同参数的调用、不要用工具名做变量名。\n",
    "  - 结构化版主要提供最小约束（定义变量再用、参数名显式），输出以 JSON 承载 Thought 和 Code，便于上层程序直接消费。\n",
    "- 执行与可读性：\n",
    "  - 标准版输出包含 Thought/Code/Observation 叙事，适合人工旁观调试、流式展示和教学场景。\n",
    "  - 结构化版输出紧凑、机器友好，适合日志解析、监控、对接编排器或二次路由。\n",
    "- 选择建议：\n",
    "  - 需要确定性结构、便于程序解析/回放/审计时，用 `use_structured_outputs_internally=True`（结构化版）。\n",
    "  - 需要人类可读的逐步对话、希望看到 code_block 包裹和 Observation 明细，或想依赖模板中的链式提示时，用 `use_structured_outputs_internally=False`（标准版）。\n",
    "- 常见踩坑提示：\n",
    "  - 标准版务必保持自定义 code_block 标签，否则解析失败；非结构化工具调用不要在同一块紧接依赖下一工具输出。\n",
    "  - 结构化版的 JSON 输出中请将可执行代码写在 `code` 字段，仍需显式 `final_answer(...)` 收口；确保不要把工具名当变量名。\n",
    "- 性能与上下文：标准版提示体积更大，可能略增 token 开销；结构化版更紧凑，节省上下文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeaf91e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9a193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
