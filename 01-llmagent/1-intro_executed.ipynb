{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ba230c",
   "metadata": {},
   "source": [
    "# Easy Agent Tutorial\n",
    "This notebook file provide three examples of using LLM based agents with different tool sets.\n",
    "\n",
    "Prerequisites:\n",
    "- Python 3.10+\n",
    "- Install required packages:\n",
    "  ```bash\n",
    "  pip install requests python-dotenv langchain-community langchain-openai langchain langgraph \"smolagents[mcp, gradio]\" faiss-cpu tenacity pyyaml\n",
    "  ```\n",
    "\n",
    "## Task Description\n",
    "\n",
    "如README所述，该项目应用三种方案，从不同的角度实现了agentic RAG的功能。为了演示，这一次我们将会构建一个信安四大会的查询，来进行感兴趣论文的搜索以及基于题目选择合适的会议进行投稿。\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a91a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们将使用dblp数据集来进行演示。首先下载信安四大会2025年的会议论文数据：\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"dblp_sec_papers.json\"):\n",
    "    with requests.Session() as sess:\n",
    "        url = \"https://dblp.org/search/publ/api\"\n",
    "        params = {\"q\": \"security\", \"format\": \"json\"}\n",
    "        tocs = {\n",
    "            \"ndss\": \"toc:db/conf/ndss/ndss2025.bht:\",\n",
    "            \"sp\": \"toc:db/conf/sp/sp2025.bht:\",\n",
    "            \"ccs\": \"toc:db/conf/ccs/ccs2025.bht:\",\n",
    "            \"usenix\": \"toc:db/conf/uss/uss2025.bht:\",\n",
    "        }\n",
    "        papers = []\n",
    "        for k, v in tocs.items():\n",
    "            response = sess.get(url, params={\"q\": v, \"h\": 1000, \"format\": \"json\"})\n",
    "            data = response.json()\n",
    "            data = data[\"result\"][\"hits\"][\"hit\"]\n",
    "            papers.extend(data)\n",
    "        with open(f\"dblp_sec_papers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    with open(f\"dblp_sec_papers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        papers = json.load(f)\n",
    "papers = [x[\"info\"] for x in papers]\n",
    "titles = [f\"[{x['venue']} {x['year']}] {x['title']}\" for x in papers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ea572",
   "metadata": {},
   "source": [
    "在进行下一步之前，需要对环境进行一些配置，\n",
    "创建一个`.env`文件，并添加以下内容：\n",
    "\n",
    "-   示例 1：OpenAI 官方服务\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "HTTPS_PROXY=your_proxy\n",
    "MODEL_NAME=gpt-5.1\n",
    "EMB_MODEL_NAME=text-embedding-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff07b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建并保存向量数据库\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"db\" not in globals():\n",
    "    db = FAISS.from_texts(titles, OpenAIEmbeddings())\n",
    "db.save_local(\"faiss_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352eb4d",
   "metadata": {},
   "source": [
    "## 方案0： 一切奇迹的始发点——传统工具调用\n",
    "\n",
    "在开始之前，先看一下传统的工具调用大概长啥样，有怎样的优缺点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: llm安全 论文\n",
      "Searching for keyword: llm安全\n",
      "Searching for keyword: 论文\n",
      "kw='llm安全 论文' appended.\n",
      "下面给你整理一批近两年「LLM 安全」相关的代表性论文，主要来自 USENIX Security / IEEE S&P (Oakland) / CCS / NDSS 2025，方便你查阅和做文献综述。我先按主题简单分一下类，再列核心论文（中文说明 + 原题方便你搜索）。\n",
      "\n",
      "如果你说明更具体的方向（比如：越狱攻击、RAG 安全、隐私泄露、模型水印、医疗场景等），我可以帮你做更细的分类和阅读笔记。\n",
      "\n",
      "---\n",
      "\n",
      "## 一、LLM 越狱与对抗攻击 / 防御\n",
      "\n",
      "**攻击与基准**\n",
      "\n",
      "1. **Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 关键词：自动化越狱、任务级脆弱性、攻击与防御基准\n",
      "\n",
      "2. **PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 用模糊测试（fuzzing）自动探索提示空间，实现高效隐蔽的越狱。\n",
      "\n",
      "3. **Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 分析对齐后模型拒答边界中“不起眼”的薄弱点，属于越狱与安全对齐交叉。\n",
      "\n",
      "4. **Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image Generation Models**  \n",
      "   - 会议：IEEE S&P (SP) 2025  \n",
      "   - 虽面向文生图模型，但方法与 LLM 越狱类似，对 agent / 多模态安全也有参考价值。\n",
      "\n",
      "**防御与自我保护**\n",
      "\n",
      "5. **SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 提出让 LLM 自身参与监测与拦截越狱提示的框架，强调实用性。\n",
      "\n",
      "6. **Cloak, Honey, Trap: Proactive Defenses Against LLM Agents**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 对 LLM agents 的主动防御：伪装（Cloak）、蜜罐（Honey）、陷阱（Trap），适合做 agent 安全综述的材料。\n",
      "\n",
      "---\n",
      "\n",
      "## 二、RAG / LLM 应用与代码生成安全\n",
      "\n",
      "7. **Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection**  \n",
      "   - 会议：CCS 2025  \n",
      "   - 关注 RAG + 代码生成场景，通过“注入安全知识”降低生成代码漏洞率。\n",
      "\n",
      "8. **Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 研究向知识库注入“阻断文档”，干扰 RAG 输出的攻击方式。\n",
      "\n",
      "9. **Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search**  \n",
      "   - 会议：USENIX Security 2025  \n",
      "   - 针对 LLM 驱动的搜索（搜索引擎+聊天），系统量化安全风险并提出缓解方法。\n",
      "\n",
      "---\n",
      "\n",
      "## 三、隐私泄露、个人信息与医疗场景\n",
      "\n",
      "10. **Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain**  \n",
      "    - 会议：CCS 2025  \n",
      "    - 医疗场景下的隐私攻击与防御，是做“LLM+医疗+隐私”研究的核心文献。\n",
      "\n",
      "11. **Evaluating LLM-based Personal Information Extraction and Countermeasures**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - LLM 如何抽取个人信息（PII），以及如何防止这种抽取。\n",
      "\n",
      "12. **PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 提一个框架来检测 LLM 是否存在隐私泄露行为（训练数据记忆等）。\n",
      "\n",
      "13. **Depth Gives a False Sense of Privacy: LLM Internal States Inversion**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 通过“内部状态反演”重构输入，说明网络深度不等于隐私安全。\n",
      "\n",
      "14. **MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs**  \n",
      "    - 会议：CCS 2025  \n",
      "    - 面向 MoE 结构 LLM 的侧信道攻击，揭示隐私风险。\n",
      "\n",
      "15. **Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 更偏用户研究，分析在心理健康场景下用户对安全与隐私的担忧。\n",
      "\n",
      "16. **Prevalence Overshadows Concerns? Understanding Chinese Users' Privacy Awareness and Expectations Towards LLM-Based Healthcare Consultation**  \n",
      "    - 会议：IEEE S&P 2025  \n",
      "    - 面向中国用户的隐私意识与预期，关注 LLM 医疗咨询场景。\n",
      "\n",
      "---\n",
      "\n",
      "## 四、模型水印、溯源与指纹\n",
      "\n",
      "17. **Provably Robust Multi-bit Watermarking for AI-generated Text**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 理论上有鲁棒性保证的多比特文本水印方法。\n",
      "\n",
      "18. **A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 针对树环水印的去除攻击，适合作为“水印攻防”配套阅读。\n",
      "\n",
      "19. **RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models**  \n",
      "    - 会议：CCS 2025  \n",
      "    - 为 RAG 输出设计黑盒水印机制。\n",
      "\n",
      "20. **LLMmap: Fingerprinting for Large Language Models**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 为不同 LLM 建立“指纹”，用于识别模型来源、检测模型窃用。\n",
      "\n",
      "21. **TracLLM: A Generic Framework for Attributing Long Context LLMs**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 面向长上下文 LLM 的归因框架，与模型知识产权保护、安全审计相关。\n",
      "\n",
      "---\n",
      "\n",
      "## 五、Web 接入、插件 / Agent 与生态安全\n",
      "\n",
      "22. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 系统分析 LLM 接入互联网（浏览网页、调用 API）后的新型攻击面。\n",
      "\n",
      "23. **On the (In)Security of LLM App Stores**  \n",
      "    - 会议：IEEE S&P 2025  \n",
      "    - 研究 “LLM 应用商店” 模式中的安全问题（恶意 app、权限滥用等）。\n",
      "\n",
      "24. **The Philosopher's Stone: Trojaning Plugins of Large Language Models**  \n",
      "    - 会议：NDSS 2025  \n",
      "    - 针对 LLM 插件后门 / 木马攻击，极适合用来做“插件安全”专题。\n",
      "\n",
      "25. **Cloak, Honey, Trap: Proactive Defenses Against LLM Agents**  \n",
      "    - 前面提过，聚焦 LLM agents 主动防御。\n",
      "\n",
      "---\n",
      "\n",
      "## 六、安全对齐与微调风险\n",
      "\n",
      "26. **Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  \n",
      "    - 会议：IEEE S&P 2025  \n",
      "    - 分析微调过程中安全对齐丢失的问题，并提出缓解策略。\n",
      "\n",
      "27. **Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 研究激活近似（如量化、裁剪）如何破坏安全对齐，引入新的安全漏洞。\n",
      "\n",
      "28. **Unlocking the Power of Differentially Private Zeroth-order Optimization for Fine-tuning LLMs**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 使用差分隐私 + 零阶优化微调 LLM，在隐私保护与性能之间权衡。\n",
      "\n",
      "---\n",
      "\n",
      "## 七、LLM 在安全领域的“正向应用”\n",
      "\n",
      "29. **Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM Analysis**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 用 LLM 对日志进行 Tree-of-Thought 分析并自动生成补丁。\n",
      "\n",
      "30. **LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 将 Code Property Graph 与 LLM 结合，用于漏洞检测。\n",
      "\n",
      "31. **Generating API Parameter Security Rules with LLM for API Misuse Detection**  \n",
      "    - 会议：NDSS 2025  \n",
      "    - 利用 LLM 自动生成 API 参数安全规则，发现 API 滥用。\n",
      "\n",
      "32. **The Midas Touch: Triggering the Capability of LLMs for RM-API Misuse Detection**  \n",
      "    - 会议：NDSS 2025  \n",
      "    - 用 LLM 识别云资源管理类 API 的误用问题。\n",
      "\n",
      "33. **LLMPirate: LLMs for Black-box Hardware IP Piracy**  \n",
      "    - 会议：NDSS 2025  \n",
      "    - 虽是“反面”应用：利用 LLM 做硬件 IP 盗版，也是理解 LLM 在安全中的攻防潜力的代表作。\n",
      "\n",
      "---\n",
      "\n",
      "## 八、政策、合规与用户研究（与 LLM 安全相关）\n",
      "\n",
      "34. **Evaluating Privacy Policies under Modern Privacy Laws At Scale: An LLM-Based Automated Approach**  \n",
      "    - 会议：USENIX Security 2025  \n",
      "    - 利用 LLM 自动分析隐私政策与现代隐私法的合规性。\n",
      "\n",
      "35. **A Wall Behind A Wall: Emerging Regional Censorship in China**  \n",
      "    - 会议：IEEE S&P 2025  \n",
      "    - 与区域审查 / 内容封锁相关，部分内容使用 LLM 做分析，适合作为“内容安全与审查”参照。\n",
      "\n",
      "36. **The Power of Words: A Comprehensive Analysis of Rationales and Their Effects on Users' Permission Decisions**  \n",
      "    - 会议：NDSS 2025  \n",
      "    - 虽然不完全是 LLM，但对理解“模型解释如何影响用户安全决策”很有帮助。\n",
      "\n",
      "---\n",
      "\n",
      "## 如何获取这些论文\n",
      "\n",
      "1. 去各大会议官网：  \n",
      "   - USENIX Security 2025: https://www.usenix.org/conference/usenixsecurity25  \n",
      "   - IEEE S&P 2025: https://www.ieee-security.org/TC/SP2025/  \n",
      "   - CCS 2025: https://www.sigsac.org/ccs/CCS2025/  \n",
      "   - NDSS 2025: https://www.ndss-symposium.org/ndss2025/\n",
      "\n",
      "2. 在 Google Scholar / Semantic Scholar 中搜索：  \n",
      "   - 关键字示例：  \n",
      "     - `\"PAPILLON\" jailbreak LLM`  \n",
      "     - `\"SelfDefend\" jailbreaking LLM`  \n",
      "     - `\"LLMmap\" fingerprinting`  \n",
      "     - `\"Can Personal Health Information Be Secured in LLM\"`  \n",
      "\n",
      "---\n",
      "\n",
      "如果你告诉我你的具体研究方向（比如：做某类攻击方法、做防御机制设计、做水印/溯源、做医疗场景隐私等），我可以帮你：  \n",
      "- 挑 5–10 篇最核心的  \n",
      "- 给每篇写中文要点（威胁模型 / 方法 / 主要结论 / 可借鉴点）  \n",
      "- 帮你构思一个文献综述/开题报告的大纲。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import dotenv\n",
    "import tenacity\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "from openai.types import *\n",
    "from openai.types.chat import *\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def raw_toolcall():\n",
    "    client = OpenAI()\n",
    "    db = FAISS.load_local(\n",
    "        \"faiss_db\",\n",
    "        OpenAIEmbeddings(),\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    tools_def = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"query_paperdb\",\n",
    "            \"description\": \"accept keyword and return related information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"kw\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The keyword to query the database\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"kw\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def real_ask(question: str):\n",
    "        from openai.types.responses.response_input_param import Message\n",
    "\n",
    "        input_msgs: list[Message] = [\n",
    "            {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful research assistant. When given a question, you must first decide if you need to query the database to get relevant information. If so, use the tool 'query_paperdb' with appropriate keywords extracted from the question. After getting the information, provide a comprehensive answer based on both the retrieved information and your own knowledge.\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"message\",\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ]\n",
    "        resp = client.responses.create(\n",
    "            model=\"gpt-5.1\",\n",
    "            tools=tools_def,\n",
    "            input=input_msgs,\n",
    "            tool_choice=\"required\",\n",
    "        )\n",
    "        for toolcall in resp.output:\n",
    "            if toolcall.type != \"function_call\":\n",
    "                continue\n",
    "            if toolcall.name == \"query_paperdb\":\n",
    "                kw = json.loads(toolcall.arguments)[\"kw\"]\n",
    "                print(f\"Keyword: {kw}\")\n",
    "                docs = []\n",
    "                for skw in kw.split():\n",
    "                    if not (skw := skw.strip()):\n",
    "                        continue\n",
    "                    print(f\"Searching for keyword: {skw}\")\n",
    "                    docs.extend(db.similarity_search(skw, k=30))\n",
    "                rag_result = \"\\n\".join([doc.page_content for doc in docs])\n",
    "                input_msgs.append(toolcall)\n",
    "                input_msgs.append(\n",
    "                    {\n",
    "                        \"type\": \"function_call_output\",\n",
    "                        \"call_id\": toolcall.call_id,\n",
    "                        \"output\": str(rag_result),\n",
    "                    }\n",
    "                )\n",
    "                print(f\"{kw=} appended.\")\n",
    "        if input_msgs[-1][\"type\"] == \"function_call_output\":\n",
    "            resp = client.responses.create(\n",
    "                model=\"gpt-5.1\",\n",
    "                input=input_msgs,\n",
    "                # tools=tools_def,\n",
    "                stream=True,\n",
    "            )\n",
    "            for chunk in resp:\n",
    "                if chunk.type == \"response.output_text.delta\":\n",
    "                    print(chunk.delta, end=\"\", flush=True)\n",
    "            print()\n",
    "        else:\n",
    "            print(resp.output_text)\n",
    "\n",
    "    while True:\n",
    "        inp = input(\"=> \")\n",
    "        if not inp.strip():\n",
    "            break\n",
    "        real_ask(inp)\n",
    "\n",
    "\n",
    "raw_toolcall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0f2b1",
   "metadata": {},
   "source": [
    "### 传统工具调用的优缺点\n",
    "\n",
    "- 优点：直接用模型原生的 function/tool 调用协议，链路短、开销低，JSON Schema 参数校验清晰。\n",
    "- 优点：可以精确控制何时调用工具、使用 `tool_choice` 等参数强制执行，消息格式透明、便于调试和流式输出。\n",
    "- 优点：依赖少，不绑框架，易于插入到现有服务或与其他编排层组合。\n",
    "- 缺点：需要手写对话状态管理、工具输入输出拼接，容易出错且样板代码多。\n",
    "- 缺点：缺少自动规划/多步推理、重试、fallback 等封装能力，复杂流程要自行实现。\n",
    "- 缺点：与特定模型/协议耦合，换提供方或多模型时需适配；安全性与数据清洗（如反序列化、去重）也要自管。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888af19",
   "metadata": {},
   "source": [
    "## 方案1： SmolAgent::CodeAgent\n",
    "\n",
    "SmolAgent::CodeAgent 拥有两种提示模板，使用`use_structured_outputs_internally`参数进行控制。\n",
    "- 核心循环：两条线路都遵循多步推理，但输出格式不同。\n",
    "  - 标准版（`code_agent.yaml`，`use_structured_outputs_internally=False`）：每步输出 Thought→Code→Observation，代码必须包裹在自定义 `{{code_block_opening_tag}}`/`{{code_block_closing_tag}}` 中；中间结果用 `print` 进入 Observation，最终用 `final_answer` 收束。\n",
    "  - 结构化版（`structured_code_agent.yaml`，`use_structured_outputs_internally=True`）：每步输出固定 JSON `{ \"thought\": \"...\", \"code\": \"...\" }`，默认不需要自定义 code_block 标签；解析稳定、便于日志/回放。\n",
    "- 规划 scaffold：两者都要求先做 facts survey + high-level plan（initial_plan / update_plan），但标准版在文本里更强调「分步打印、避免链式依赖」。\n",
    "- 工具调用与链式策略：\n",
    "  - 标准版明确区分“有 JSON schema 的工具可以链式调用，非结构化工具应先 `print` 再下一步用”，并警示不要重做相同参数的调用、不要用工具名做变量名。\n",
    "  - 结构化版主要提供最小约束（定义变量再用、参数名显式），输出以 JSON 承载 Thought 和 Code，便于上层程序直接消费。\n",
    "- 执行与可读性：\n",
    "  - 标准版输出包含 Thought/Code/Observation 叙事，适合人工旁观调试、流式展示和教学场景。\n",
    "  - 结构化版输出紧凑、机器友好，适合日志解析、监控、对接编排器或二次路由。\n",
    "- 选择建议：\n",
    "  - 需要确定性结构、便于程序解析/回放/审计时，用 `use_structured_outputs_internally=True`（结构化版）。\n",
    "  - 需要人类可读的逐步对话、希望看到 code_block 包裹和 Observation 明细，或想依赖模板中的链式提示时，用 `use_structured_outputs_internally=False`（标准版）。\n",
    "- 常见踩坑提示：\n",
    "  - 标准版务必保持自定义 code_block 标签，否则解析失败；非结构化工具调用不要在同一块紧接依赖下一工具输出。\n",
    "  - 结构化版的 JSON 输出中请将可执行代码写在 `code` 字段，仍需显式 `final_answer(...)` 收口；确保不要把工具名当变量名。\n",
    "- 性能与上下文：标准版提示体积更大，可能略增 token 开销；结构化版更紧凑，节省上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d330b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">You are a helpful research assistant.</span>                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">When given a question, you must query the database to get relevant information.</span>                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Use the tools with appropriate arguments derived from the question.</span>                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">After getting the information, provide a comprehensive answer based on both the retrieved information.</span>          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Output the final answer in markdown wrapped in final_answer().</span>                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">查询llm安全相关的论文</span>                                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIModel - gpt-5.1 ─────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYou are a helpful research assistant.\u001b[0m                                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhen given a question, you must query the database to get relevant information.\u001b[0m                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUse the tools with appropriate arguments derived from the question.\u001b[0m                                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mAfter getting the information, provide a comprehensive answer based on both the retrieved information.\u001b[0m          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mOutput the final answer in markdown wrapped in final_answer().\u001b[0m                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1m查询llm安全相关的论文\u001b[0m                                                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIModel - gpt-5.1 \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0fc5d2791e43fe850a9a2053eabf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">from</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> typing </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> List</span><span style=\"background-color: #272822\">                                                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Step 1: Query the paper database for LLM safety related keywords (in English and Chinese)</span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLM safety|large language model safety|AI alignment|红队 攻击 LLM|大模型 安全 防护|jailbreak defense\"</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">papers_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> query_paperdb(kw</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw)</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(papers_info)</span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtyping\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mList\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Step 1: Query the paper database for LLM safety related keywords (in English and Chinese)\u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLM safety|large language model safety|AI alignment|红队 攻击 LLM|大模型 安全 防护|jailbreak defense\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpapers_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery_paperdb\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpapers_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed at line 'from typing import List' due to: InterpreterError: Import from typing is not </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">allowed. Authorized imports are: ['queue', 'collections', 'unicodedata', 'datetime', 'stat', 'random', 're', </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'math', 'statistics', 'time', 'itertools'\\]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode execution failed at line 'from typing import List' due to: InterpreterError: Import from typing is not \u001b[0m\n",
       "\u001b[1;31mallowed. Authorized imports are: ['queue', 'collections', 'unicodedata', 'datetime', 'stat', 'random', 're', \u001b[0m\n",
       "\u001b[1;31m'math', 'statistics', 'time', 'itertools'\\]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.12 seconds| Input tokens: 2,096 | Output tokens: 94]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.12 seconds| Input tokens: 2,096 | Output tokens: 94]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0afca150b4b4fe4bf7ffec51b1ea058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 第一步：用中英文关键词查询与 LLM 安全相关的论文</span><span style=\"background-color: #272822\">                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLM safety|large language model safety|foundation model safety|jailbreak|red teaming|prompt </span><span style=\"background-color: #272822\">             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">injection|alignment|对齐 安全|大语言模型 安全|大模型 安全 攻击 防御|越狱 攻击 LLM\"</span><span style=\"background-color: #272822\">                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">papers_raw </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> query_paperdb(kw</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">kw)</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(papers_raw)</span><span style=\"background-color: #272822\">                                                                                              </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# 第一步：用中英文关键词查询与 LLM 安全相关的论文\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLM safety|large language model safety|foundation model safety|jailbreak|red teaming|prompt \u001b[0m\u001b[48;2;39;40;34m             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34minjection|alignment|对齐 安全|大语言模型 安全|大模型 安全 攻击 防御|越狱 攻击 LLM\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpapers_raw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery_paperdb\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpapers_raw\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for keyword: LLM safety\n",
      "Searching for keyword: large language model safety\n",
      "Searching for keyword: foundation model safety\n",
      "Searching for keyword: jailbreak\n",
      "Searching for keyword: red teaming\n",
      "Searching for keyword: prompt injection\n",
      "Searching for keyword: alignment\n",
      "Searching for keyword: 对齐 安全\n",
      "Searching for keyword: 大语言模型 安全\n",
      "Searching for keyword: 大模型 安全 攻击 防御\n",
      "Searching for keyword: 越狱 攻击 LLM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "[USENIX Security Symposium 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain.\n",
       "[SP 2025] On the (In)Security of LLM App Stores.\n",
       "[USENIX Security Symposium 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: \n",
       "Comprehensive Analysis and Defense.\n",
       "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in \n",
       "AI Web Search.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&amp;apos; \n",
       "Refusal Boundaries.\n",
       "[USENIX Security Symposium 2025] EchoLLM: LLM-Augmented Acoustic Eavesdropping Attack on Bone Conduction Headphones\n",
       "with mmWave Radar.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language \n",
       "Models.\n",
       "[USENIX Security Symposium 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language \n",
       "Models on Generated Data.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] From Purity to Peril: Backdooring Merged Models From &amp;quot;Harmless&amp;quot; Benign \n",
       "Components.\n",
       "[USENIX Security Symposium 2025] Lancet: A Formalization Framework for Crash and Exploit Pathology.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models \n",
       "with Limited Data.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] SoK: A Framework and Guide for Human-Centered Threat Modeling in Security and Privacy Research.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image \n",
       "Generation Models.\n",
       "[USENIX Security Symposium 2025] ChoiceJacking: Compromising Mobile Devices through Malicious Chargers like a \n",
       "Decade ago.\n",
       "[USENIX Security Symposium 2025] TapTrap: Animation-Driven Tapjacking on Android.\n",
       "[USENIX Security Symposium 2025] Red Bleed: A Pragmatic Near-Infrared Presentation Attack on Facial Biometric \n",
       "Authentication Systems.\n",
       "[USENIX Security Symposium 2025] &amp;quot;Threat modeling is very formal, it&amp;apos;s very technical, and also very hard\n",
       "to do correctly&amp;quot;: Investigating Threat Modeling Practices in Open-Source Software Projects.\n",
       "[USENIX Security Symposium 2025] Revisiting Training-Inference Trigger Intensity in Backdoor Attacks.\n",
       "[USENIX Security Symposium 2025] Beyond Exploit Scanning: A Functional Change-Driven Approach to Remote Software \n",
       "Version Identification.\n",
       "[USENIX Security Symposium 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for \n",
       "Static Analysis Result Verification.\n",
       "[USENIX Security Symposium 2025] Towards a Re-evaluation of Data Forging Attacks in Practice.\n",
       "[USENIX Security Symposium 2025] Principled and Automated Approach for Investigating AR/VR Attacks.\n",
       "[USENIX Security Symposium 2025] IDFuzz: Intelligent Directed Grey-box Fuzzing.\n",
       "[USENIX Security Symposium 2025] High Stakes, Low Certainty: Evaluating the Efficacy of High-Level Indicators of \n",
       "Compromise in Ransomware Attribution.\n",
       "[USENIX Security Symposium 2025] Cyber-Physical Deception Through Coordinated IoT Honeypots.\n",
       "[USENIX Security Symposium 2025] StruQ: Defending Against Prompt Injection with Structured Queries.\n",
       "[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.\n",
       "[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models.\n",
       "[USENIX Security Symposium 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services.\n",
       "[USENIX Security Symposium 2025] Private Investigator: Extracting Personally Identifiable Information from Large \n",
       "Language Models Using Optimized Prompts.\n",
       "[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection \n",
       "Attacks via the Fine-Tuning Interface.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[CCS 2025] PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and \n",
       "Verification.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&amp;apos; \n",
       "Refusal Boundaries.\n",
       "[SP 2025] Characterizing Robocalls with Multiple Vantage Points.\n",
       "[CCS 2025] Elastic Restaking Networks: United we fall, (partially) divided we stand.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] Connecting the Extra Dots (Contexts): Correlating External Information about Point of Interest for Attack\n",
       "Investigation.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] Flexway O-Sort: Enclave-Friendly and Optimal Oblivious Sorting.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Efficient 2PC for Constant Round Secure Equality Testing and Comparison.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] Found in Translation: A Generative Language Modeling Approach to Memory Access \n",
       "Pattern Attacks.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language \n",
       "Models.\n",
       "[CCS 2025] Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving.\n",
       "[USENIX Security Symposium 2025] Pretender: Universal Active Defense against Diffusion Finetuning Attacks.\n",
       "[CCS 2025] Poster: Black-box Attacks on Multimodal Large Language Models through Adversarial ICC Profiles.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[NDSS 2025] Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[SP 2025] Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom \n",
       "Attacks.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM \n",
       "Analysis.\n",
       "[NDSS 2025] LLMPirate: LLMs for Black-box Hardware IP Piracy.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[CCS 2025] MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "[USENIX Security Symposium 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain.\n",
       "[SP 2025] On the (In)Security of LLM App Stores.\n",
       "[USENIX Security Symposium 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: \n",
       "Comprehensive Analysis and Defense.\n",
       "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in \n",
       "AI Web Search.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&apos; \n",
       "Refusal Boundaries.\n",
       "[USENIX Security Symposium 2025] EchoLLM: LLM-Augmented Acoustic Eavesdropping Attack on Bone Conduction Headphones\n",
       "with mmWave Radar.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language \n",
       "Models.\n",
       "[USENIX Security Symposium 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language \n",
       "Models on Generated Data.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] From Purity to Peril: Backdooring Merged Models From &quot;Harmless&quot; Benign \n",
       "Components.\n",
       "[USENIX Security Symposium 2025] Lancet: A Formalization Framework for Crash and Exploit Pathology.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models \n",
       "with Limited Data.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] SoK: A Framework and Guide for Human-Centered Threat Modeling in Security and Privacy Research.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image \n",
       "Generation Models.\n",
       "[USENIX Security Symposium 2025] ChoiceJacking: Compromising Mobile Devices through Malicious Chargers like a \n",
       "Decade ago.\n",
       "[USENIX Security Symposium 2025] TapTrap: Animation-Driven Tapjacking on Android.\n",
       "[USENIX Security Symposium 2025] Red Bleed: A Pragmatic Near-Infrared Presentation Attack on Facial Biometric \n",
       "Authentication Systems.\n",
       "[USENIX Security Symposium 2025] &quot;Threat modeling is very formal, it&apos;s very technical, and also very hard\n",
       "to do correctly&quot;: Investigating Threat Modeling Practices in Open-Source Software Projects.\n",
       "[USENIX Security Symposium 2025] Revisiting Training-Inference Trigger Intensity in Backdoor Attacks.\n",
       "[USENIX Security Symposium 2025] Beyond Exploit Scanning: A Functional Change-Driven Approach to Remote Software \n",
       "Version Identification.\n",
       "[USENIX Security Symposium 2025] From Alarms to Real Bugs: Multi-target Multi-step Directed Greybox Fuzzing for \n",
       "Static Analysis Result Verification.\n",
       "[USENIX Security Symposium 2025] Towards a Re-evaluation of Data Forging Attacks in Practice.\n",
       "[USENIX Security Symposium 2025] Principled and Automated Approach for Investigating AR/VR Attacks.\n",
       "[USENIX Security Symposium 2025] IDFuzz: Intelligent Directed Grey-box Fuzzing.\n",
       "[USENIX Security Symposium 2025] High Stakes, Low Certainty: Evaluating the Efficacy of High-Level Indicators of \n",
       "Compromise in Ransomware Attribution.\n",
       "[USENIX Security Symposium 2025] Cyber-Physical Deception Through Coordinated IoT Honeypots.\n",
       "[USENIX Security Symposium 2025] StruQ: Defending Against Prompt Injection with Structured Queries.\n",
       "[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.\n",
       "[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models.\n",
       "[USENIX Security Symposium 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services.\n",
       "[USENIX Security Symposium 2025] Private Investigator: Extracting Personally Identifiable Information from Large \n",
       "Language Models Using Optimized Prompts.\n",
       "[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection \n",
       "Attacks via the Fine-Tuning Interface.\n",
       "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
       "[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[CCS 2025] PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and \n",
       "Verification.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[USENIX Security Symposium 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs&apos; \n",
       "Refusal Boundaries.\n",
       "[SP 2025] Characterizing Robocalls with Multiple Vantage Points.\n",
       "[CCS 2025] Elastic Restaking Networks: United we fall, (partially) divided we stand.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[SP 2025] Connecting the Extra Dots (Contexts): Correlating External Information about Point of Interest for Attack\n",
       "Investigation.\n",
       "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical \n",
       "Privacy-Preserving Machine Learning.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[USENIX Security Symposium 2025] Flexway O-Sort: Enclave-Friendly and Optimal Oblivious Sorting.\n",
       "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
       "[USENIX Security Symposium 2025] Efficient 2PC for Constant Round Secure Equality Testing and Comparison.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
       "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across \n",
       "Modalities.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[USENIX Security Symposium 2025] Found in Translation: A Generative Language Modeling Approach to Memory Access \n",
       "Pattern Attacks.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[USENIX Security Symposium 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language \n",
       "Models.\n",
       "[CCS 2025] Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving.\n",
       "[USENIX Security Symposium 2025] Pretender: Universal Active Defense against Diffusion Finetuning Attacks.\n",
       "[CCS 2025] Poster: Black-box Attacks on Multimodal Large Language Models through Adversarial ICC Profiles.\n",
       "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
       "[NDSS 2025] Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables.\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models.\n",
       "[CCS 2025] Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[SP 2025] Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom \n",
       "Attacks.\n",
       "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak \n",
       "Attack.\n",
       "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
       "[USENIX Security Symposium 2025] Logs In, Patches Out: Automated Vulnerability Repair via Tree-of-Thought LLM \n",
       "Analysis.\n",
       "[NDSS 2025] LLMPirate: LLMs for Black-box Hardware IP Piracy.\n",
       "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
       "[CCS 2025] MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 8.75 seconds| Input tokens: 4,503 | Output tokens: 257]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 8.75 seconds| Input tokens: 4,503 | Output tokens: 257]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e8a4f64d624972afa3730b0f22e72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 这里我们已经拿到了大量与“大语言模型（LLM）安全”高度相关的论文题目</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 接下来不再调用工具，而是基于这些结果，按主题给你做一个中文综述与阅读建议。</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">summary </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">r\"\"\"</span><span style=\"background-color: #272822\">                                                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  </span><span style=\"background-color: #272822\">                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">（会议缩写：SP=IEEE S&amp;P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）</span><span style=\"background-color: #272822\">                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 一、总体图景：LLM 安全/对齐的主要研究方向</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">从检索结果看，LLM 安全论文大致可以分为几大类：</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **越狱（Jailbreak）与红队攻击**</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 同时研究防御策略，例如对齐增强、概念级控制等。</span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **提示注入（Prompt Injection）、Prompt 窃取和推断**</span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 提出结构化查询、对齐优化等防御机制。</span><span style=\"background-color: #272822\">                                                                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。</span><span style=\"background-color: #272822\">                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **对齐与“安全失配”（Safety Misalignment）**</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **隐私与数据泄露**</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 从模型中提取个人敏感信息（PII）。</span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 成员推断（membership inference）与训练数据保护。</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">5. **后门、模型合并与生成数据风险**</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 大模型后门攻击与检测方法。</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 模型合并（model merging）中的隐含安全风险。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">6. **面向特定群体/应用的安全**</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 面向青少年/未成年人内容安全。</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。</span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">7. **多模态安全（Vision-Language / Text-to-Image）**</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 图文大模型识别不安全内容的能力评估与改进。</span><span style=\"background-color: #272822\">                                                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 文本到图像模型的越狱与防御。</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。</span><span style=\"background-color: #272822\">                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 二、越狱攻击与防御（Jailbreak / Red-teaming）</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**攻击方法类：**</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  </span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。</span><span style=\"background-color: #272822\">             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  </span><span style=\"background-color: #272822\">                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。</span><span style=\"background-color: #272822\">                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Text-to-Image Generation Models**  </span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。</span><span style=\"background-color: #272822\">                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**防御方法类：**</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  </span><span style=\"background-color: #272822\">        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。</span><span style=\"background-color: #272822\">                                              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 实践性：强调可部署性，对工程系统启发大。</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Analysis and Manipulation**  </span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。</span><span style=\"background-color: #272822\">                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">for LLMs**  </span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。</span><span style=\"background-color: #272822\">                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 三、提示注入、提示窃取与结构化防御</span><span style=\"background-color: #272822\">                                                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**攻击与测量：**</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  </span><span style=\"background-color: #272822\">                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。</span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 有助于理解 Prompt 作为“知识资产”的威胁模型。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  </span><span style=\"background-color: #272822\">             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  </span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 场景：多方推理、分布式推理中的 prompt 泄露风险。</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Injection Attacks via the Fine-Tuning Interface**  </span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。</span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**防御与对齐优化：**</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  </span><span style=\"background-color: #272822\">                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。</span><span style=\"background-color: #272822\">                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  </span><span style=\"background-color: #272822\">                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。</span><span style=\"background-color: #272822\">                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。</span><span style=\"background-color: #272822\">                                                </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 四、安全对齐与“(In)Security / Misalignment”问题</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[NDSS 2025] Safety Misalignment Against Large Language Models**  </span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  </span><span style=\"background-color: #272822\">                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 关注：下游微调是否、以及如何破坏原有安全对齐。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提供：策略来在微调时尽量保持对齐属性。</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  </span><span style=\"background-color: #272822\">           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 帮助你理解对齐机制的脆弱点。</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。</span><span style=\"background-color: #272822\">                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Analysis and Defense**  </span><span style=\"background-color: #272822\">                                                                                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。</span><span style=\"background-color: #272822\">                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 五、隐私、成员推断与敏感数据安全</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Domain**  </span><span style=\"background-color: #272822\">                                                                                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 适合：对医疗+大模型交叉感兴趣的方向。</span><span style=\"background-color: #272822\">                                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Models Using Optimized Prompts**  </span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 展示：优化提示如何从看似安全的模型中挖出 PII。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  </span><span style=\"background-color: #272822\"> </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。</span><span style=\"background-color: #272822\">                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Generated Data**  </span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。</span><span style=\"background-color: #272822\">                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  </span><span style=\"background-color: #272822\">                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。</span><span style=\"background-color: #272822\">                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 六、后门、模型合并和生成数据风险</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 利用嵌入空间特性构造可迁移/跨触发的后门。</span><span style=\"background-color: #272822\">                                                                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components**  </span><span style=\"background-color: #272822\">        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。</span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  </span><span style=\"background-color: #272822\">                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 七、青少年/特定群体与应用场景安全</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Search**  </span><span style=\"background-color: #272822\">                                                                                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。</span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] On the (In)Security of LLM App Stores**  </span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 类似“插件商店 / LLM App Store”的生态安全问题。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  </span><span style=\"background-color: #272822\">                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 八、多模态与图像相关安全（T2I / VLM）</span><span style=\"background-color: #272822\">                                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Text-to-Image Generation Models**  </span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  </span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。</span><span style=\"background-color: #272822\">                </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 九、如何系统入门 LLM 安全研究？</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **宏观理解 LLM 安全问题空间**</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 阅读：  </span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  </span><span style=\"background-color: #272822\">                                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  </span><span style=\"background-color: #272822\">                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。</span><span style=\"background-color: #272822\">         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 实践：  </span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  </span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">     - 改写系统提示、添加规则模型/过滤器，观察防御效果。</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **扩展到隐私&amp;后门等更深层次问题**</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。</span><span style=\"background-color: #272822\">                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。</span><span style=\"background-color: #272822\">                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **关注“上线场景”的综合安全**</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。</span><span style=\"background-color: #272822\">                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。</span><span style=\"background-color: #272822\">                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 十、如果你想要一份“快速论文清单”</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：</span><span style=\"background-color: #272822\">                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  </span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  </span><span style=\"background-color: #272822\">                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  </span><span style=\"background-color: #272822\">                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **SelfDefend / JBShield**（了解防御思路）  </span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  </span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  </span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  </span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">8. **YouthSafe**（了解“面向特定人群”的安全基准构建）</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">如果你告诉我：  </span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 你更偏「攻击」还是「防御」；  </span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 更想做「理论/算法」还是「工程系统/落地」；  </span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  </span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。</span><span style=\"background-color: #272822\">                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"\"\"</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(summary)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# 这里我们已经拿到了大量与“大语言模型（LLM）安全”高度相关的论文题目\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# 接下来不再调用工具，而是基于这些结果，按主题给你做一个中文综述与阅读建议。\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mr\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  \u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m（会议缩写：SP=IEEE S&P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）\u001b[0m\u001b[48;2;39;40;34m                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 一、总体图景：LLM 安全/对齐的主要研究方向\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m从检索结果看，LLM 安全论文大致可以分为几大类：\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **越狱（Jailbreak）与红队攻击**\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 同时研究防御策略，例如对齐增强、概念级控制等。\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **提示注入（Prompt Injection）、Prompt 窃取和推断**\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 提出结构化查询、对齐优化等防御机制。\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **对齐与“安全失配”（Safety Misalignment）**\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **隐私与数据泄露**\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 从模型中提取个人敏感信息（PII）。\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 成员推断（membership inference）与训练数据保护。\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m5. **后门、模型合并与生成数据风险**\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 大模型后门攻击与检测方法。\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 模型合并（model merging）中的隐含安全风险。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m6. **面向特定群体/应用的安全**\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 面向青少年/未成年人内容安全。\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m7. **多模态安全（Vision-Language / Text-to-Image）**\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 图文大模型识别不安全内容的能力评估与改进。\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 文本到图像模型的越狱与防御。\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 二、越狱攻击与防御（Jailbreak / Red-teaming）\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**攻击方法类：**\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  \u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  \u001b[0m\u001b[48;2;39;40;34m                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mText-to-Image Generation Models**  \u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**防御方法类：**\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 实践性：强调可部署性，对工程系统启发大。\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mAnalysis and Manipulation**  \u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mfor LLMs**  \u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。\u001b[0m\u001b[48;2;39;40;34m                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 三、提示注入、提示窃取与结构化防御\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**攻击与测量：**\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  \u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。\u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 有助于理解 Prompt 作为“知识资产”的威胁模型。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  \u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 场景：多方推理、分布式推理中的 prompt 泄露风险。\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mInjection Attacks via the Fine-Tuning Interface**  \u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。\u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**防御与对齐优化：**\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  \u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。\u001b[0m\u001b[48;2;39;40;34m                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  \u001b[0m\u001b[48;2;39;40;34m                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。\u001b[0m\u001b[48;2;39;40;34m                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 四、安全对齐与“(In)Security / Misalignment”问题\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[NDSS 2025] Safety Misalignment Against Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  \u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 关注：下游微调是否、以及如何破坏原有安全对齐。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提供：策略来在微调时尽量保持对齐属性。\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 帮助你理解对齐机制的脆弱点。\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m Refusal Boundaries**  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mAnalysis and Defense**  \u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 五、隐私、成员推断与敏感数据安全\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mDomain**  \u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 适合：对医疗+大模型交叉感兴趣的方向。\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mModels Using Optimized Prompts**  \u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 展示：优化提示如何从看似安全的模型中挖出 PII。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mGenerated Data**  \u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  \u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 六、后门、模型合并和生成数据风险\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 利用嵌入空间特性构造可迁移/跨触发的后门。\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mHarmless\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m Benign Components**  \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。\u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  \u001b[0m\u001b[48;2;39;40;34m                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 七、青少年/特定群体与应用场景安全\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mSearch**  \u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] On the (In)Security of LLM App Stores**  \u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 类似“插件商店 / LLM App Store”的生态安全问题。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  \u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 八、多模态与图像相关安全（T2I / VLM）\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mText-to-Image Generation Models**  \u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  \u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 九、如何系统入门 LLM 安全研究？\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **宏观理解 LLM 安全问题空间**\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 阅读：  \u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  \u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  \u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。\u001b[0m\u001b[48;2;39;40;34m         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 实践：  \u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  \u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m     - 改写系统提示、添加规则模型/过滤器，观察防御效果。\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **扩展到隐私&后门等更深层次问题**\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **关注“上线场景”的综合安全**\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。\u001b[0m\u001b[48;2;39;40;34m                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。\u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 十、如果你想要一份“快速论文清单”\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  \u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  \u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  \u001b[0m\u001b[48;2;39;40;34m                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **SelfDefend / JBShield**（了解防御思路）  \u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  \u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  \u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  \u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m8. **YouthSafe**（了解“面向特定人群”的安全基准构建）\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m如果你告诉我：  \u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 你更偏「攻击」还是「防御」；  \u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 更想做「理论/算法」还是「工程系统/落地」；  \u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  \u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">（会议缩写：SP=IEEE S&amp;P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 一、总体图景：LLM 安全/对齐的主要研究方向</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">从检索结果看，LLM 安全论文大致可以分为几大类：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **越狱（Jailbreak）与红队攻击**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 同时研究防御策略，例如对齐增强、概念级控制等。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **提示注入（Prompt Injection）、Prompt 窃取和推断**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 提出结构化查询、对齐优化等防御机制。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **对齐与“安全失配”（Safety Misalignment）**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **隐私与数据泄露**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 从模型中提取个人敏感信息（PII）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 成员推断（membership inference）与训练数据保护。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">5. **后门、模型合并与生成数据风险**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 大模型后门攻击与检测方法。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 模型合并（model merging）中的隐含安全风险。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">6. **面向特定群体/应用的安全**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 面向青少年/未成年人内容安全。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">7. **多模态安全（Vision-Language / Text-to-Image）**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 图文大模型识别不安全内容的能力评估与改进。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 文本到图像模型的越狱与防御。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 二、越狱攻击与防御（Jailbreak / Red-teaming）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**攻击方法类：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Text-to-Image Generation Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**防御方法类：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 实践性：强调可部署性，对工程系统启发大。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Analysis and Manipulation**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">LLMs**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 三、提示注入、提示窃取与结构化防御</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**攻击与测量：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 有助于理解 Prompt 作为“知识资产”的威胁模型。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 场景：多方推理、分布式推理中的 prompt 泄露风险。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Injection Attacks via the Fine-Tuning Interface**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**防御与对齐优化：**</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 四、安全对齐与“(In)Security / Misalignment”问题</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[NDSS 2025] Safety Misalignment Against Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 关注：下游微调是否、以及如何破坏原有安全对齐。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提供：策略来在微调时尽量保持对齐属性。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 帮助你理解对齐机制的脆弱点。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Analysis and Defense**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 五、隐私、成员推断与敏感数据安全</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Domain**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 适合：对医疗+大模型交叉感兴趣的方向。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language Models </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Using Optimized Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 展示：优化提示如何从看似安全的模型中挖出 PII。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Generated Data**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 六、后门、模型合并和生成数据风险</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 利用嵌入空间特性构造可迁移/跨触发的后门。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 七、青少年/特定群体与应用场景安全</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search** </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] On the (In)Security of LLM App Stores**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 类似“插件商店 / LLM App Store”的生态安全问题。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 八、多模态与图像相关安全（T2I / VLM）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Text-to-Image Generation Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 九、如何系统入门 LLM 安全研究？</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **宏观理解 LLM 安全问题空间**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 阅读：  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 实践：  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">     - 改写系统提示、添加规则模型/过滤器，观察防御效果。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **扩展到隐私&amp;后门等更深层次问题**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **关注“上线场景”的综合安全**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 十、如果你想要一份“快速论文清单”</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **SelfDefend / JBShield**（了解防御思路）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">8. **YouthSafe**（了解“面向特定人群”的安全基准构建）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">如果你告诉我：  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 你更偏「攻击」还是「防御」；  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 更想做「理论/算法」还是「工程系统/落地」；  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m下面根据你查询到的 llm 安全相关论文列表，按研究方向给出一个结构化综述和阅读建议。  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m（会议缩写：SP=IEEE S&P，CCS=ACM CCS，NDSS=NDSS，USENIX=USENIX Security）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 一、总体图景：LLM 安全/对齐的主要研究方向\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m从检索结果看，LLM 安全论文大致可以分为几大类：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **越狱（Jailbreak）与红队攻击**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 设计更强的越狱攻击方法，系统性评估主流模型的防御能力。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 提出自动化测试/模糊测试（fuzzing）等技术来发现安全策略的弱点。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 同时研究防御策略，例如对齐增强、概念级控制等。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m2. **提示注入（Prompt Injection）、Prompt 窃取和推断**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 针对多轮对话、RAG、代理（agent）、提示服务等场景下的注入攻击。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 提出结构化查询、对齐优化等防御机制。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 研究 Prompt Stealing / Prompt Inference 等推断攻击。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m3. **对齐与“安全失配”（Safety Misalignment）**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 模型尽管被对齐，但在复杂场景中仍会出现不安全输出。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 分析对齐在微调、工具调用、RAG 等场景中的退化问题。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 探讨如何在保持能力的同时，减少安全对齐的“遗失”。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m4. **隐私与数据泄露**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 从模型中提取个人敏感信息（PII）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 成员推断（membership inference）与训练数据保护。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 在医疗等敏感领域评估 LLM 的隐私风险与防御。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m5. **后门、模型合并与生成数据风险**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 大模型后门攻击与检测方法。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 模型合并（model merging）中的隐含安全风险。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 使用生成数据微调可能引入伪隐私、偏见和安全问题。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m6. **面向特定群体/应用的安全**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 面向青少年/未成年人内容安全。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 面向代码生成、漏洞检测等安全相关任务的 LLM 应用。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 面向搜索、Web 访问、应用商店等“LLM 上线”场景的新风险。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m7. **多模态安全（Vision-Language / Text-to-Image）**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 图文大模型识别不安全内容的能力评估与改进。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 文本到图像模型的越狱与防御。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m下面以具体论文为例，给你一个“按方向+代表论文”的阅读清单和简要要点，方便你搭建知识体系。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 二、越狱攻击与防御（Jailbreak / Red-teaming）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**攻击方法类：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：利用“成对的双胞胎提示（Twin Prompts）”构造更隐蔽、稳定的越狱攻击。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 贡献：系统展现现有对齐策略如何在 Twin Prompts 下被绕过，说明安全策略对某些结构化提示不够鲁棒。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 你可以关注：提示模式（pattern），对齐策略在结构对称下的失效机理。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：将“模糊测试（fuzzing）”思想引入越狱提示搜索，自动生成多样化攻击提示。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 特点：高效率、可自动运行，适合作为安全评估工具链的一部分。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：多轮对话式越狱，先从温和问题逐渐诱导模型突破安全边界。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 价值：提示多轮交互引入的新攻击面，单轮审查防御不足。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mText-to-Image Generation Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 虽是图像生成模型，但方法论（代理+fuzz）可迁移到 LLM 红队自动化。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**防御方法类：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 核心：利用 LLM 自身能力构造自我防御管线（如自审查、自修正）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 实践性：强调可部署性，对工程系统启发大。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mAnalysis and Manipulation**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 关键点：通过“激活概念（activated concepts）”视角理解和控制模型内部对有害内容的表征。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 特点：更偏向可解释+内部干预，而非仅靠外部规则过滤。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mLLMs**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 贡献：不只给攻击，还给出一套评价不同防御配置的基准框架，适合做横向对比实验。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 三、提示注入、提示窃取与结构化防御\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**攻击与测量：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 聚焦：通过反向推理、黑盒查询等方式从在线服务中窃取高价值系统提示（system prompt / jailbreak prompt）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 有助于理解 Prompt 作为“知识资产”的威胁模型。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 场景：多方推理、分布式推理中的 prompt 泄露风险。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mInjection Attacks via the Fine-Tuning Interface**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 看点：利用“微调接口”实现类似 prompt injection 的效果，说明开放微调 API 本身就是一个新的攻击面。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**防御与对齐优化：**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] StruQ: Defending Against Prompt Injection with Structured Queries**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 思路：将自然语言指令映射为结构化查询/调用，减少模型对“任意文本”的被动执行，限制攻击面。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 方法：通过偏好优化（preference optimization）训练模型拒绝 prompt injection 类的恶意模式。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 视角：博弈论建模攻击者/防御者，从策略优化角度设计检测机制。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 四、安全对齐与“(In)Security / Misalignment”问题\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[NDSS 2025] Safety Misalignment Against Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 重点：系统性分析模型在不同任务/场景下的安全表现与对齐目标的偏差。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 有利于理解“为什么看起来合规的模型在某些边缘情况仍危险”。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 关注：下游微调是否、以及如何破坏原有安全对齐。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提供：策略来在微调时尽量保持对齐属性。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 视角：攻击者如何“解对齐”（unlearning alignment），让模型重新产生有害输出。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 帮助你理解对齐机制的脆弱点。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 分析：拒绝边界上的“隐蔽弱点”，即在边界附近的微小改写就能绕过安全策略。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mAnalysis and Defense**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 说明：在模型压缩、加速等工程过程（如激活近似）中，安全对齐可能被破坏。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 五、隐私、成员推断与敏感数据安全\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mDomain**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 关注：PHI（个人健康信息）在 LLM 中的泄露风险与防御方案。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 适合：对医疗+大模型交叉感兴趣的方向。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language Models \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mUsing Optimized Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 展示：优化提示如何从看似安全的模型中挖出 PII。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 成员推断：攻击者仅凭输出标签/概率，判断某样本是否在训练集中。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mGenerated Data**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 指出：用“生成数据”当作看似无隐私风险的训练数据，也可能带来安全与隐私的伪安全感。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[NDSS 2025] DLBox: New Model Training Framework for Protecting Training Data**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 更偏平台层、系统层保护训练数据，适合做“系统安全+LLM”的同学阅读。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 六、后门、模型合并和生成数据风险\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 利用嵌入空间特性构造可迁移/跨触发的后门。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 说明：看似安全的子模型在合并后可能触发新的后门行为，对于“社区模型合并”有重要启示。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提出一种“逆向目标”的后门扫描手段，用于检测大模型是否含有恶意触发。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 七、青少年/特定群体与应用场景安全\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提供：针对青少年的安全基准与防护模型，关注内容适龄、心理影响、风险暴露等。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search** \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 场景：LLM 驱动的搜索与网页访问，为“上网+大模型”类产品提供风险分析。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Supporting Human Raters with the Detection of Harmful Content Using Large Language Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 探讨：如何用 LLM 辅助人类内容审核，而不是完全替代。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] On the (In)Security of LLM App Stores**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 类似“插件商店 / LLM App Store”的生态安全问题。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 强调：LLM 接入 Web 后，攻击面从“对话文本”扩展到整个互联网资源。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 八、多模态与图像相关安全（T2I / VLM）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[SP 2025] Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mText-to-Image Generation Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **[CCS 2025] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 共同关注：文本到图像模型的越狱与内容安全控制，方法和评测框架部分对多模态 LLM 也有参考价值。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- **[USENIX 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 检查：图文大模型在识别多模态不安全内容时存在的“模态差距”。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 九、如何系统入门 LLM 安全研究？\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m给你一个可执行的学习路线（假设已有一定 LLM/深度学习基础）：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **宏观理解 LLM 安全问题空间**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 阅读：  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - *Safety Misalignment Against Large Language Models* (NDSS 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs* (USENIX 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 目标：理解威胁模型、攻击面、典型攻击/防御类别。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m2. **深入一个具体方向（例如：Jailbreak + Prompt Injection）**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 攻击视角（TwinBreak, PAPILLON, Crescendo multi-turn, PRSA, Prompt Stealing/Inversion 一类论文）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 防御视角（SelfDefend, JBShield, StruQ, SecAlign, DataSentinel）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 实践：  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - 在开源模型（如 LLaMA / Qwen / ChatGLM 等）上复现简单越狱攻击。  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m     - 改写系统提示、添加规则模型/过滤器，观察防御效果。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m3. **扩展到隐私&后门等更深层次问题**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 阅读 PHI/PII 提取、membership inference、EmbedX、BAIT 等论文。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 结合差分隐私/安全训练框架（DLBox 等）考虑系统层防护。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m4. **关注“上线场景”的综合安全**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - Web-enabled LLM、LLM app store、安全搜索、RAG 系统中的 prompt injection 与数据投毒。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 对接企业落地场景（客服、搜索、代码生成、安全分析等）。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m## 十、如果你想要一份“快速论文清单”\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m若你时间有限，可以优先看（对 LLM 安全整体理解帮助最大）：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **Safety Misalignment Against Large Language Models** (NDSS 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2. **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs** (USENIX 2025)  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m3. **TwinBreak / PAPILLON / Crescendo Multi-Turn**（任选 1–2 篇了解现代 jailbreak 形态）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m4. **SelfDefend / JBShield**（了解防御思路）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m5. **StruQ + SecAlign + DataSentinel**（提示注入防御代表）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m6. **Can Personal Health Information Be Secured in LLM?**（看隐私）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m7. **EmbedX / From Purity to Peril / BAIT**（看后门与模型合并风险）  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m8. **YouthSafe**（了解“面向特定人群”的安全基准构建）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m如果你告诉我：  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 你更偏「攻击」还是「防御」；  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 更想做「理论/算法」还是「工程系统/落地」；  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 是否有特定应用场景（搜索、RAG、代码、安全分析、教育等），  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m我可以基于这批论文帮你定制一个更细致的**研究选题和实验路线图**。\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 53.20 seconds| Input tokens: 10,080 | Output tokens: 4,046]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 53.20 seconds| Input tokens: 10,080 | Output tokens: 4,046]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.token_usage=TokenUsage(input_tokens=10080, output_tokens=4046, total_tokens=14126) res.timing=Timing(start_time=1765350604.0020978, end_time=1765350669.0835783, duration=65.08148050308228)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from smolagents import CodeAgent, OpenAIServerModel, tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"faiss_db\",\n",
    "    OpenAIEmbeddings(),\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a helpful research assistant.\n",
    "When given a question, you must query the database to get relevant information.\n",
    "Use the tools with appropriate arguments derived from the question.\n",
    "After getting the information, provide a comprehensive answer based on both the retrieved information.\n",
    "Output the final answer in markdown wrapped in final_answer().\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "@tool\n",
    "def query_paperdb(kw: str) -> str:\n",
    "    \"\"\"\n",
    "    Accept keywords and return related paper titles, multiple keywords should be separated by '|'.\n",
    "    This tool should be called before output any realworld-related information.\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword to query the database\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for skw in kw.split(\"|\"):\n",
    "        if not (skw := skw.strip()):\n",
    "            continue\n",
    "        print(f\"Searching for keyword: {skw}\")\n",
    "        docs.extend(db.similarity_search(skw, k=10))\n",
    "    rag_result = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return rag_result\n",
    "\n",
    "\n",
    "agent = CodeAgent(\n",
    "    model=OpenAIServerModel(\"gpt-5.1\"),\n",
    "    tools=[query_paperdb],\n",
    "    stream_outputs=True,\n",
    "    # use_structured_outputs_internally=True, # True for structured_code_agent.yaml, False for code_agent.yaml\n",
    ")\n",
    "\n",
    "# from Gradio_UI import GradioUI\n",
    "\n",
    "# GradioUI(agent).launch()\n",
    "\n",
    "_ = agent.run(f\"{sys_prompt}\\n\\n{input(\"=> \")}\", return_full_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5978dc",
   "metadata": {},
   "source": [
    "## 方案2： LangGraph\n",
    "\n",
    "以下代码块用 LangGraph 重新实现「检索→工具调用→回答」的代理流程，强调显式状态机与可视化路由的优势：\n",
    "\n",
    "\n",
    "\n",
    "- 基础资源：启动时加载 `.env`，用 `FAISS.load_local` + `OpenAIEmbeddings` 复用前面构建的向量库，确保跨会话复现检索结果。\n",
    "\n",
    "- 系统提示：`sys_prompt` 要求回答前必须调用检索工具，并用 `final_answer(...)` 收口；保持与前两种方案一致，便于对比 token 与行为。\n",
    "\n",
    "- 工具实现：`@tool` 装饰的 `query_paperdb` 按 `|` 分割多关键词，每个关键词检索 top-10，stdout 打印检索日志，返回拼接文本作为 RAG 证据。\n",
    "\n",
    "- 模型与绑定：`ChatOpenAI(model=\"gpt-5.1\", verbose=True)` 绑定工具得到 `chat_with_tools`，LangGraph 自动根据模型给出的 `tool_calls` 决定是否进入工具节点。\n",
    "\n",
    "- 状态与路由：`AgentState` 仅维护 `messages`（用 `add_messages` 合并），`StateGraph` 定义两个节点：\n",
    "  - `assistant`：调用大模型，可能产出工具调用或直接回答。\n",
    "  - `tools`：`ToolNode` 执行 `query_paperdb` 并把结果写回消息。\n",
    " 通过 `tools_condition` 条件边在两节点间循环，直到模型不再请求工具。\n",
    "\n",
    "- 编译与运行：`builder.compile()` 得到 `agent`，随后用 `SystemMessage+HumanMessage` 作为初始消息调用 `agent.invoke`。执行结束后按顺序打印每段回复，便于观察模型与工具交替的对话片段。\n",
    "\n",
    "- 适用场景：当需要显式控制多轮工具路由、插入额外节点（如过滤、重排序、裁剪上下文）或做流程可视化/监控时，LangGraph 方案更易扩展；相较裸工具调用（方案0）和 SmolAgent（方案1），它在可编排性与可观测性上更强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for keyword: LLM安全\n",
      "Searching for keyword: large language model security\n",
      "Searching for keyword: prompt injection\n",
      "Searching for keyword: jailbreak defense\n",
      "Searching for keyword: model alignment\n",
      "Searching for keyword: AI安全\n",
      "Part 0: \n",
      "You are a helpful research assistant.\n",
      "When given a question, you must query the database to get relevant information.\n",
      "Use the tools with appropriate arguments derived from the question.\n",
      "After getting the information, provide a comprehensive answer based on both the retrieved information.\n",
      "Output the final answer in markdown wrapped in final_answer().\n",
      "\n",
      "\n",
      "Part 1: \n",
      "查询llm安全相关的论文\n",
      "\n",
      "\n",
      "Part 2: \n",
      "\n",
      "\n",
      "\n",
      "Part 3: \n",
      "[SP 2025] On the (In)Security of LLM App Stores.\n",
      "[USENIX Security Symposium 2025] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs.\n",
      "[CCS 2025] Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain.\n",
      "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
      "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
      "[USENIX Security Symposium 2025] Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense.\n",
      "[USENIX Security Symposium 2025] Evaluating LLM-based Personal Information Extraction and Countermeasures.\n",
      "[SP 2025] Prevalence Overshadows Concerns? Understanding Chinese Users&apos; Privacy Awareness and Expectations Towards LLM-Based Healthcare Consultation.\n",
      "[USENIX Security Symposium 2025] Evaluating Privacy Policies under Modern Privacy Laws At Scale: An LLM-Based Automated Approach.\n",
      "[USENIX Security Symposium 2025] EchoLLM: LLM-Augmented Acoustic Eavesdropping Attack on Bone Conduction Headphones with mmWave Radar.\n",
      "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
      "[NDSS 2025] From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection.\n",
      "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
      "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
      "[CCS 2025] YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models.\n",
      "[USENIX Security Symposium 2025] LLMmap: Fingerprinting for Large Language Models.\n",
      "[SP 2025] BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.\n",
      "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models.\n",
      "[USENIX Security Symposium 2025] Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data.\n",
      "[USENIX Security Symposium 2025] EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models.\n",
      "[USENIX Security Symposium 2025] StruQ: Defending Against Prompt Injection with Structured Queries.\n",
      "[SP 2025] On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts.\n",
      "[CCS 2025] SecAlign: Defending Against Prompt Injection with Preference Optimization.\n",
      "[SP 2025] DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.\n",
      "[SP 2025] Prompt Inversion Attack Against Collaborative Inference of Large Language Models.\n",
      "[USENIX Security Symposium 2025] PRSA: Prompt Stealing Attacks against Real-World Prompt Services.\n",
      "[USENIX Security Symposium 2025] Private Investigator: Extracting Personally Identifiable Information from Large Language Models Using Optimized Prompts.\n",
      "[SP 2025] Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection Attacks via the Fine-Tuning Interface.\n",
      "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
      "[CCS 2025] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks.\n",
      "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
      "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation.\n",
      "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs.\n",
      "[USENIX Security Symposium 2025] PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs.\n",
      "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
      "[USENIX Security Symposium 2025] Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack.\n",
      "[SP 2025] Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.\n",
      "[USENIX Security Symposium 2025] Understanding How Users Prepare for and React to Smartphone Theft.\n",
      "[USENIX Security Symposium 2025] ChoiceJacking: Compromising Mobile Devices through Malicious Chargers like a Decade ago.\n",
      "[SP 2025] On the (In)Security of LLM App Stores.\n",
      "[SP 2025] Alleviating the Fear of Losing Alignment in LLM Fine-tuning.\n",
      "[NDSS 2025] Safety Misalignment Against Large Language Models.\n",
      "[CCS 2025] PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and Verification.\n",
      "[USENIX Security Symposium 2025] Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models.\n",
      "[USENIX Security Symposium 2025] Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical Privacy-Preserving Machine Learning.\n",
      "[USENIX Security Symposium 2025] Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities.\n",
      "[CCS 2025] Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning.\n",
      "[NDSS 2025] A New PPML Paradigm for Quantized Models.\n",
      "[NDSS 2025] SHAFT: Secure, Handy, Accurate and Fast Transformer Inference.\n",
      "[USENIX Security Symposium 2025] Membership Inference Attacks Against Vision-Language Models.\n",
      "[CCS 2025] AISec &apos;25: 18th ACM Workshop on Artificial Intelligence and Security.\n",
      "[USENIX Security Symposium 2025] Aion: Robust and Efficient Multi-Round Single-Mask Secure Aggregation Against Malicious Participants.\n",
      "[CCS 2025] Poster: Towards Intelligent Assurance for Autonomous AI Pentesters: Concurrent Compliance Auditing and Self-Augmentation via Execution Trace Analysis.\n",
      "[CCS 2025] SaTS &apos;25: The 3rd ACM Workshop on Security and Privacy of AI-Empowered Mobile Super Apps.\n",
      "[USENIX Security Symposium 2025] Analyzing the AI Nudification Application Ecosystem.\n",
      "[USENIX Security Symposium 2025] Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search.\n",
      "[SP 2025] Exploring Parent-Child Perceptions on Safety in Generative AI: Concerns, Mitigation Strategies, and Design Implications.\n",
      "[SP 2025] Understanding Users&apos; Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms.\n",
      "[USENIX Security Symposium 2025] Neural Invisibility Cloak: Concealing Adversary in Images via Compromised AI-driven Image Signal Processing.\n",
      "[USENIX Security Symposium 2025] Provably Robust Multi-bit Watermarking for AI-generated Text.\n",
      "\n",
      "\n",
      "Part 4: \n",
      "final_answer()\n",
      "\n",
      "下面按主题帮你梳理一些近两年“LLM 安全（Large Language Model Security）”相关的代表性工作，方便你根据方向继续检索与阅读。括号中给出会议与年份，基本都是顶会（S&P/USENIX/CCS/NDSS 等）：\n",
      "\n",
      "---\n",
      "\n",
      "## 1. LLM 应用与生态系统安全\n",
      "\n",
      "- **LLM 应用商店与应用生态**\n",
      "  - *On the (In)Security of LLM App Stores* （SP 2025）  \n",
      "    研究 LLM App Store 场景的安全性，类似移动应用商店的安全问题在 LLM 生态中的再现，关注恶意应用、权限滥用等风险。\n",
      "\n",
      "- **Web 联网 LLM 的新威胁**\n",
      "  - *When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs*（USENIX Security 2025）  \n",
      "    系统性分析“能上网”的 LLM 带来的新威胁，如错误信息放大、数据抓取、跨站攻击链路等。\n",
      "\n",
      "- **LLM 搜索与聚合服务安全**\n",
      "  - *Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search*（USENIX Security 2025）  \n",
      "    分析 LLM 驱动的搜索引擎中的安全/内容风险，并提出缓解方法。\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Prompt 注入、Prompt 盗取与 RAG 安全\n",
      "\n",
      "- **Prompt 注入攻击与防御**\n",
      "  - *DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks*（SP 2025）  \n",
      "    从博弈论角度设计 Prompt 注入检测方法。\n",
      "  - *StruQ: Defending Against Prompt Injection with Structured Queries*（USENIX Security 2025）  \n",
      "    使用结构化查询来隔离/控制 LLM 对外部数据的访问，抵御 Prompt 注入。\n",
      "  - *SecAlign: Defending Against Prompt Injection with Preference Optimization*（CCS 2025）  \n",
      "    使用偏好优化（preference optimization）提高模型在 Prompt 注入场景的鲁棒性。\n",
      "  - *Prompt Inversion Attack Against Collaborative Inference of Large Language Models*（SP 2025）  \n",
      "    对协同推理框架发起 Prompt 反演攻击，推断系统内部 Prompt。\n",
      "  - *Prompt Inference Attack on Distributed Large Language Model Inference Frameworks*（CCS 2025）  \n",
      "    针对分布式推理框架的 Prompt 推断攻击。\n",
      "\n",
      "- **Prompt 盗取 / Prompt Stealing**\n",
      "  - *On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts*（SP 2025）  \n",
      "    评估针对真实环境 Prompt 的偷窃攻击效果。\n",
      "  - *PRSA: Prompt Stealing Attacks against Real-World Prompt Services*（USENIX Security 2025）  \n",
      "    面向商用 Prompt 服务的系统化 Prompt 盗取攻击。\n",
      "\n",
      "- **RAG / 代码生成场景安全**\n",
      "  - *Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection*（CCS 2025）  \n",
      "    针对带检索增强的代码生成（RAG+Code），通过注入安全知识降低生成代码中的安全漏洞风险。\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Jailbreak / 对齐失效 与 对抗防御\n",
      "\n",
      "- **Jailbreak 攻击与基准**\n",
      "  - *Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs*（USENIX Security 2025）  \n",
      "    提出自动化 Jailbreak 攻击与防御评测基准，从“任务级别”分析对齐漏洞。\n",
      "  - *PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs*（USENIX Security 2025）  \n",
      "    利用模糊测试自动寻找高效、隐蔽的 Jailbreak 提示。\n",
      "  - *TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts*（USENIX Security 2025）  \n",
      "    使用“双提示”结构绕过安全对齐。\n",
      "  - *Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack*（USENIX Security 2025）  \n",
      "    多轮对话逐步升级的“渐进式” Jailbreak 攻击。\n",
      "  - *Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts*（SP 2025 / USENIX Security 2025，列出两次）  \n",
      "    针对文生图模型的 Prompt 式 Jailbreak，区分但与 LLM 安全问题高度相关。\n",
      "\n",
      "- **Jailbreak 防御**\n",
      "  - *SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner*（USENIX Security 2025）  \n",
      "    让模型自我检测、自我防御 Jailbreak 的设计。\n",
      "  - *JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation*（USENIX Security 2025）  \n",
      "    基于“激活概念”分析与调控实现防御，类似在内部表示层面做安全控制。\n",
      "\n",
      "- **对齐稳定性与失效**\n",
      "  - *Alleviating the Fear of Losing Alignment in LLM Fine-tuning*（SP 2025）  \n",
      "    关注微调过程中对齐丢失问题，并提出缓解策略。\n",
      "  - *Safety Misalignment Against Large Language Models*（NDSS 2025）  \n",
      "    系统性分析安全对齐失配（safety misalignment）的形式与成因。\n",
      "  - *Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models*（USENIX Security 2025）  \n",
      "    展示如何“去除”模型的安全对齐能力，即对齐可被逆向/解除。\n",
      "  - *Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense*（USENIX Security 2025）  \n",
      "    分析在推理加速中使用激活近似会带来安全对齐脆弱性。\n",
      "\n",
      "---\n",
      "\n",
      "## 4. 隐私、PII、成员推断与数据泄露\n",
      "\n",
      "- **LLM 中隐私攻击与防御（特别是医疗）**\n",
      "  - *Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain*（CCS 2025）  \n",
      "    针对医疗场景（PHI）的隐私攻击与防御机制。\n",
      "\n",
      "- **PII 提取与隐私泄露**\n",
      "  - *Private Investigator: Extracting Personally Identifiable Information from Large Language Models Using Optimized Prompts*（USENIX Security 2025）  \n",
      "    用优化提示从模型中抽取 PII 的攻击方法。\n",
      "  - *Evaluating LLM-based Personal Information Extraction and Countermeasures*（USENIX Security 2025）  \n",
      "    评估基于 LLM 的个人信息抽取能力及对应防御。\n",
      "\n",
      "- **成员推断 / 模型隐私**\n",
      "  - *Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models*（USENIX Security 2025）  \n",
      "    仅利用“标签/输出”进行成员推断攻击，针对预训练 LLM。\n",
      "  - *Membership Inference Attacks Against Vision-Language Models*（USENIX Security 2025）  \n",
      "    扩展到多模态模型的成员推断问题。\n",
      "\n",
      "- **生成数据与隐私错觉**\n",
      "  - *Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data*（USENIX Security 2025）  \n",
      "    质疑“用模型生成数据再微调就隐私安全”的假设，指出潜在隐患。\n",
      "\n",
      "- **政策与合规**\n",
      "  - *Evaluating Privacy Policies under Modern Privacy Laws At Scale: An LLM-Based Automated Approach*（USENIX Security 2025）  \n",
      "    使用 LLM 自动评估隐私政策是否符合现代隐私法规。\n",
      "\n",
      "---\n",
      "\n",
      "## 5. 后门、指纹、水印与模型识别\n",
      "\n",
      "- **后门扫描与攻击**\n",
      "  - *BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target*（SP 2025）  \n",
      "    通过“反演攻击目标”的方式检测 LLM 中的后门。\n",
      "  - *EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models*（USENIX Security 2025）  \n",
      "    基于 embedding 的跨触发后门攻击。\n",
      "\n",
      "- **模型指纹与识别**\n",
      "  - *LLMmap: Fingerprinting for Large Language Models*（USENIX Security 2025）  \n",
      "    给 LLM 做“指纹识别”，用于模型同一性检测、知识产权保护等。\n",
      "\n",
      "- **水印与版权保护**\n",
      "  - *Provably Robust Multi-bit Watermarking for AI-generated Text*（USENIX Security 2025）  \n",
      "    面向文本生成的多比特水印方案，强调可证明的鲁棒性。\n",
      "  - *PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and Verification*（CCS 2025）  \n",
      "    在对齐数据集（偏好数据）中植入水印，实现数据版权保护。\n",
      "\n",
      "---\n",
      "\n",
      "## 6. 特定应用安全：密码、漏洞检测、医疗等\n",
      "\n",
      "- **密码学与口令安全**\n",
      "  - *Password Guessing Using Large Language Models*（USENIX Security 2025）  \n",
      "    评估 LLM 在密码猜测攻击中的能力及风险。\n",
      "\n",
      "- **软件漏洞检测**\n",
      "  - *From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection*（NDSS 2025）  \n",
      "    系统比较不同大模型在漏洞检测上的表现，也涉及误报/安全边界问题。\n",
      "\n",
      "- **医疗 & 用户对隐私/安全的态度**\n",
      "  - *Prevalence Overshadows Concerns? Understanding Chinese Users&apos; Privacy Awareness and Expectations Towards LLM-Based Healthcare Consultation*（SP 2025）  \n",
      "    从用户研究角度，分析中国用户对 LLM 医疗问诊的隐私认知与期望。\n",
      "\n",
      "---\n",
      "\n",
      "## 7. 儿童/青少年与用户安全感知\n",
      "\n",
      "- **青少年安全**\n",
      "  - *YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models*（CCS 2025）  \n",
      "    为青少年设计的安全基准与护航模型，关注内容适龄性等。\n",
      "\n",
      "- **普通用户对安全/隐私的认知**\n",
      "  - *Understanding Users&apos; Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms*（SP 2025）  \n",
      "    研究用户对对话式 AI 安全与隐私的看法。\n",
      "  - *Exploring Parent-Child Perceptions on Safety in Generative AI: Concerns, Mitigation Strategies, and Design Implications*（SP 2025）  \n",
      "    关注家长与孩子对生成式 AI 安全性的不同认知。\n",
      "\n",
      "---\n",
      "\n",
      "## 8. 其他与 LLM/AI 安全相关的通用工作\n",
      "\n",
      "- **安全高效推理 / PPML 等**（与 LLM 安全相关但不完全局限于 LLM）\n",
      "  - *SHAFT: Secure, Handy, Accurate and Fast Transformer Inference*（NDSS 2025）  \n",
      "    面向 Transformer 的安全与高效推理框架。\n",
      "  - *A New PPML Paradigm for Quantized Models*（NDSS 2025）  \n",
      "    定量模型下新的隐私保护机器学习范式。\n",
      "  - *Suda: An Efficient and Secure Unbalanced Data Alignment Framework for Vertical Privacy-Preserving Machine Learning*（USENIX Security 2025）  \n",
      "    等。\n",
      "\n",
      "- **安全/AI 综合 workshop**\n",
      "  - *AISec ’25: 18th ACM Workshop on Artificial Intelligence and Security*（CCS 2025）  \n",
      "    集中大量 AI+安全交叉论文。\n",
      "  - *SaTS ’25: The 3rd ACM Workshop on Security and Privacy of AI-Empowered Mobile Super Apps*（CCS 2025）  \n",
      "    关注 AI 驱动超级应用的安全与隐私。\n",
      "\n",
      "---\n",
      "\n",
      "## 如何继续检索这些论文\n",
      "\n",
      "1. 在 Google Scholar / dblp / Semantic Scholar 中搜索论文题目（或关键词+会议名）。\n",
      "2. 常用关键词组合：\n",
      "   - `\"large language model\" security`, `prompt injection`, `jailbreak attack defense`, `LLM privacy PII`, `LLM backdoor`, `alignment safety misalignment`。\n",
      "3. 若你专注某个子方向（比如 Prompt 注入防御、医疗场景隐私、RAG 安全等），可以告诉我，我可以：\n",
      "   - 帮你筛出该方向最相关的几篇；\n",
      "   - 按“问题 → 攻击思路 → 防御方案 → 开放问题”给你做一页式综述提纲，方便写 survey 或开题报告。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools import tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"faiss_db\",\n",
    "    OpenAIEmbeddings(),\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a helpful research assistant.\n",
    "When given a question, you must query the database to get relevant information.\n",
    "Use the tools with appropriate arguments derived from the question.\n",
    "After getting the information, provide a comprehensive answer based on both the retrieved information.\n",
    "Output the final answer in markdown wrapped in final_answer().\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "@tool\n",
    "def query_paperdb(kw: str) -> str:\n",
    "    \"\"\"\n",
    "    Accept keywords and return related paper titles, multiple keywords should be separated by '|'.\n",
    "    This tool should be called before output any realworld-related information.\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword to query the database\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for skw in kw.split(\"|\"):\n",
    "        if not (skw := skw.strip()):\n",
    "            continue\n",
    "        print(f\"Searching for keyword: {skw}\")\n",
    "        docs.extend(db.similarity_search(skw, k=10))\n",
    "    rag_result = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return rag_result\n",
    "\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import AIMessage, AnyMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-5.1\", verbose=True)\n",
    "chat_with_tools = chat.bind_tools([query_paperdb])\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode([query_paperdb]))\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "agent = builder.compile()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=sys_prompt),\n",
    "    HumanMessage(content=input(\"=> \")),\n",
    "]\n",
    "\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "\n",
    "for idx, part in enumerate(response[\"messages\"]):\n",
    "    print(f\"Part {idx}: \\n{part.content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562bfc9c",
   "metadata": {},
   "source": [
    "## 方案3： CodeAgent + MCP (Model Context Protocol)\n",
    "\n",
    "这一节展示如何通过 MCP 将外部工具集合挂载给 SmolAgent 的 CodeAgent，并保持与前面方案一致的“先检索后回答”规范。使用前请先在本目录启动 `mcp-1.py` 以提供本地 MCP 服务器。\n",
    "\n",
    "\n",
    "\n",
    "- 启动服务：在终端运行 `python mcp-1.py`，保持进程存活，默认监听 `http://127.0.0.1:8000/mcp`。\n",
    "\n",
    "- 工具获取：`ToolCollection.from_mcp` 以 streamable-http 方式连接服务器，`trust_remote_code=True` 允许加载远端工具实现，`structured_output=False` 让工具返回原始文本。\n",
    "\n",
    "- 代理配置：创建 `CodeAgent(model=OpenAIServerModel(\"gpt-5.1\"), tools=[*tool_collection.tools], stream_outputs=True, use_structured_outputs_internally=True)`，沿用 `sys_prompt` 强制先调用工具，再用 `final_answer(...)` 收口。\n",
    "\n",
    "- 运行流程：读取用户输入后直接 `agent.run`，中间的工具调用与结果交由 MCP 服务提供，实现与本地代码解耦、便于统一暴露多工具。\n",
    "\n",
    "- 适用场景：需要集中托管工具、复用跨项目/多语言工具链，或将工具部署为独立服务时，MCP 能作为标准化桥梁，保持模型侧最小改动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af638e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">You are a helpful research assistant.</span>                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">When given a question, you must query the database to get relevant information.</span>                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Use the tools with appropriate arguments derived from the question.</span>                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">After getting the information, provide a comprehensive answer based on both the retrieved information.</span>          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">* Output the final answer in markdown wrapped in final_answer().</span>                                                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">查询llm安全相关的论文</span>                                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIModel - gpt-5.1 ─────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYou are a helpful research assistant.\u001b[0m                                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhen given a question, you must query the database to get relevant information.\u001b[0m                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUse the tools with appropriate arguments derived from the question.\u001b[0m                                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mAfter getting the information, provide a comprehensive answer based on both the retrieved information.\u001b[0m          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1m* Output the final answer in markdown wrapped in final_answer().\u001b[0m                                                \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1m查询llm安全相关的论文\u001b[0m                                                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIModel - gpt-5.1 \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf54eb970084dca94f8c79a48277581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">papers </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> query_paperdb(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLM security|large language model security|prompt injection|model stealing|data </span><span style=\"background-color: #272822\">       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">poisoning|jailbreak attacks\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(papers)</span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpapers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery_paperdb\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLM security|large language model security|prompt injection|model stealing|data \u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mpoisoning|jailbreak attacks\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpapers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed at line 'papers = query_paperdb(\"LLM security|large language model security|prompt </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">injection|model stealing|data poisoning|jailbreak attacks\")' due to: ValueError: tool query_paperdb does not </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">support multiple positional arguments or combined positional and keyword arguments</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode execution failed at line 'papers = query_paperdb(\"LLM security|large language model security|prompt \u001b[0m\n",
       "\u001b[1;31minjection|model stealing|data poisoning|jailbreak attacks\")' due to: ValueError: tool query_paperdb does not \u001b[0m\n",
       "\u001b[1;31msupport multiple positional arguments or combined positional and keyword arguments\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 6.01 seconds| Input tokens: 1,969 | Output tokens: 87]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 6.01 seconds| Input tokens: 1,969 | Output tokens: 87]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cdb06d5a1a4c1aa0451d1968bace0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">papers </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> query_paperdb(kw</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLM security, large language model security, prompt injection, jailbreak, data </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">poisoning, model stealing, membership inference, alignment, red teaming\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(papers)</span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpapers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery_paperdb\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkw\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLM security, large language model security, prompt injection, jailbreak, data \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mpoisoning, model stealing, membership inference, alignment, red teaming\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpapers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language \n",
       "Models.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[USENIX Security Symposium 2025] PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of \n",
       "Large Language Models.\n",
       "[USENIX Security Symposium 2025] Membership Inference Attacks Against Vision-Language Models.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "[USENIX Security Symposium 2025] JBShield: Defending Large Language Models from Jailbreak Attacks through Activated\n",
       "Concept Analysis and Manipulation.\n",
       "[USENIX Security Symposium 2025] Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense \n",
       "Benchmarking for LLMs.\n",
       "[USENIX Security Symposium 2025] Password Guessing Using Large Language Models.\n",
       "[USENIX Security Symposium 2025] Towards Label-Only Membership Inference Attack against Pre-trained Large Language \n",
       "Models.\n",
       "[USENIX Security Symposium 2025] SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.\n",
       "[USENIX Security Symposium 2025] PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of \n",
       "Large Language Models.\n",
       "[USENIX Security Symposium 2025] Membership Inference Attacks Against Vision-Language Models.\n",
       "[USENIX Security Symposium 2025] TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts.\n",
       "[CCS 2025] Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.\n",
       "[USENIX Security Symposium 2025] Prompt Obfuscation for Large Language Models.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 8.02 seconds| Input tokens: 4,192 | Output tokens: 294]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 8.02 seconds| Input tokens: 4,192 | Output tokens: 294]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d908c3ff314aaaa497ed4e822052d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"\"\"## LLM 安全相关代表性论文一览（基于 2025 年顶会检索）</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">根据数据库检索结果，以下是近年在 USENIX Security、CCS </span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">等顶级安全会议上发表的、与大模型（LLM）安全密切相关的论文。为了方便理解，我按**攻击 / </span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">防御类型**进行简单归类，并用通俗中文做简要说明。</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 一、Jailbreak（越狱）攻击与防御</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">这些工作主要研究：如何绕过 LLM 的安全对齐机制（越狱），以及如何抵御此类攻击。</span><span style=\"background-color: #272822\">                                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 1. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Manipulation  </span><span style=\"background-color: #272822\">                                                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：现有 LLM 易被「越狱提示词」诱导输出违规内容（暴力、违法、隐私等）。</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- </span><span style=\"background-color: #272822\">                                                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">**核心思路**：通过分析模型内部被激活的“概念”表示，识别出与越狱/有害内容相关的激活模式，然后对这些激活进行**抑制</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">或重写**，从而在内部层面阻断越狱效果，而不仅仅依赖提示词规则过滤。</span><span style=\"background-color: #272822\">                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 安全防护不应只停留在「输入输出层」黑盒过滤，</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 利用可解释性/概念激活分析，可以在**模型内部**构建更稳健的安全防线。</span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 2. Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：现有越狱研究多集中在具体“提示技巧”，缺乏从**任务层面**系统评估不同场景下 LLM 的安全脆弱性。</span><span style=\"background-color: #272822\">        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 构建一个**自动化的 jailbreaking 攻击与防御评测平台**，</span><span style=\"background-color: #272822\">                                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 从不同任务（如代码生成、问答、对话、翻译等）层面去挖掘 LLM 的漏洞，</span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 系统比较各种攻击和防御机制在不同任务上的表现。</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - LLM 安全评估要考虑「**任务上下文**」，有的任务天生风险更大（如代码执行、系统操作等）。</span><span style=\"background-color: #272822\">                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 3. SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner  </span><span style=\"background-color: #272822\">                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：如何在不大幅修改模型架构的前提下，让 LLM 在真实系统中更好地抵御越狱？</span><span style=\"background-color: #272822\">                              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 利用 LLM 本身的推理与自我反思能力，让模型**自己检测输入是否包含越狱企图**，</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 在检测到风险时，自动调整响应策略（拒绝、重写、请求澄清等），</span><span style=\"background-color: #272822\">                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 更偏向实用工程视角，可部署在现有系统中。</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 「用 LLM 守 LLM」是一个重要趋势，</span><span style=\"background-color: #272822\">                                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 通过链式思维、自我批评、多轮审查等机制，能在工程上显著增强安全性。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 4. TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts  </span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：即便模型经过安全对齐，仍可通过巧妙设计提示组合进行绕过。</span><span style=\"background-color: #272822\">                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提出 **“Twin Prompts”（双提示）** </span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">概念：利用两个协同的提示，一个负责伪装/引导，另一个负责触发具体的有害行为，</span><span style=\"background-color: #272822\">                                    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 系统研究这类双提示结构如何破坏对齐策略，并评估各类防御机制的效果。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 实际攻击常不是“单条提示”那么简单，而是复杂的对话/多提示结构，</span><span style=\"background-color: #272822\">                                              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 防御设计时也要考虑**会话级别、多轮、多提示**的组合攻击。</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 5. Prompt Obfuscation for Large Language Models  </span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：在开放环境中部署 LLM 时，提示词（尤其是系统提示 / 指令）可能泄露敏感策略或被篡改。</span><span style=\"background-color: #272822\">                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 研究如何对提示进行**混淆（obfuscation）**，既让模型仍然理解并执行指令，又让攻击者难以直接读取或逆向出策略，</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 在提示保护、prompt leak 防护等方向上给出系统性方法。</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 除了“内容安全”，还要关注**提示本身的机密性与完整性**（prompt confidentiality &amp; integrity）。</span><span style=\"background-color: #272822\">               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 二、隐私攻击与 Membership Inference</span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 6. Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models  </span><span style=\"background-color: #272822\">              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - Membership Inference Attack（MIA）试图判断某条数据是否被用来训练模型，</span><span style=\"background-color: #272822\">                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 在很多实际场景中，攻击者只拿到“模型输出的标签/回答”，没有概率分布等详细信息。</span><span style=\"background-color: #272822\">                              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 提出针对 **“仅标签”输出（label-only）** 场景的 MIA 方法，</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 分析在预训练 LLM 上，这类攻击在何种设定下仍然可行，以及风险程度。</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 即使接口极度受限（只返回结果而非置信度），</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 训练数据隐私仍旧存在泄露风险，</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 对使用敏感数据（医疗、金融等）训练的大模型，要认真评估 MIA 风险。</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 7. Membership Inference Attacks Against Vision-Language Models  </span><span style=\"background-color: #272822\">                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：Vision-Language </span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Models（VLMs）同时处理图像与文本，其训练数据可能包含大量敏感图像（人脸、隐私场景等）。</span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 系统性分析对 VLM 的 membership inference 攻击，</span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 探究在多模态设定下，如何利用模型的图文输出特征判断某张图像/文本是否出现在训练集中。</span><span style=\"background-color: #272822\">                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 多模态大模型的隐私攻击面更大，</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 需要针对图像、文本及其组合构建新的隐私保护机制（如 DP、多模态加噪等）。</span><span style=\"background-color: #272822\">                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 三、数据与知识投毒（Poisoning）</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 8. PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models  </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - RAG（Retrieval-Augmented Generation）系统依赖外部知识库，</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 如果知识库被恶意污染（加入错误、有害、带后门的信息），LLM 生成内容会被误导甚至执行后门行为。</span><span style=\"background-color: #272822\">               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 系统研究 **“知识腐蚀攻击”**：在检索库中注入特定干扰内容，</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 分析攻击者如何通过少量恶意文档显著改变 RAG 输出，</span><span style=\"background-color: #272822\">                                                          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 并探讨潜在的检测与防御方法。</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - RAG 系统一旦上线，其**检索库安全**极其关键，</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 需要对知识库进行访问控制、数据源可信度评估、周期性审计与异常检测。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 四、代码生成与软件安全</span><span style=\"background-color: #272822\">                                                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 9. Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection  </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：CCS 2025*</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：LLM + RAG 用于代码生成时，模型可能输出不安全的代码（存在漏洞、后门、错误配置）。</span><span style=\"background-color: #272822\">                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 通过**注入安全知识**（knowledge injection），在 RAG 流程中显式检索与安全相关的最佳实践、规则、样例，</span><span style=\"background-color: #272822\">       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 让模型在生成代码时，参考这些“安全教材”，从而减少典型安全漏洞。</span><span style=\"background-color: #272822\">                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 为 LLM「补安全课」非常重要，</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 通过工具链、RAG 知识库、静态分析等组合，可显著提升代码生成安全性。</span><span style=\"background-color: #272822\">                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 五、其他安全相关方向</span><span style=\"background-color: #272822\">                                                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">### 10. Password Guessing Using Large Language Models  </span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">*会议：USENIX Security 2025*</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **问题**：评估 LLM 在**密码猜测**与破解场景中的能力与风险。</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **核心思路**：</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 利用 LLM 的语言建模能力，生成高概率/有结构的密码候选，</span><span style=\"background-color: #272822\">                                                     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 研究其在实际攻击中的效用，与传统密码猜测工具（如基于规则、概率模型的工具）对比。</span><span style=\"background-color: #272822\">                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- **启示**：</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - LLM 本身也可能被用作攻击工具（密码猜测、社工邮件撰写、钓鱼内容生成等），</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">  - 安全研究需要正视“攻防两用”的双刃剑属性。</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 六、如何继续查找 LLM 安全相关文献？</span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">如果你想系统性深入 LLM 安全方向，可以考虑以下检索关键词与方向：</span><span style=\"background-color: #272822\">                                                </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **越狱与对齐**  </span><span style=\"background-color: #272822\">                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 关键词：`LLM jailbreak`, `jailbreak attack`, `alignment`, `safety alignment`, `adversarial prompts`, </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">`prompt injection`</span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **隐私与数据安全**  </span><span style=\"background-color: #272822\">                                                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 关键词：`membership inference attack`, `training data extraction`, `privacy leakage`, `data </span><span style=\"background-color: #272822\">              </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">reconstruction`, `LLM privacy`</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **数据/知识投毒与后门**  </span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 关键词：`data poisoning`, `backdoor attack`, `RAG poisoning`, `knowledge corruption`, `prompt poisoning`</span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **RAG 与应用层安全**  </span><span style=\"background-color: #272822\">                                                                                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 关键词：`retrieval-augmented generation security`, `RAG security`, `tool-augmented LLM security`, `agents </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">security`</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">5. **多模态与代码安全**  </span><span style=\"background-color: #272822\">                                                                                      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">   - 关键词：`vision-language model security`, `secure code generation`, `LLM for security`, `security-oriented</span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">RAG`</span><span style=\"background-color: #272822\">                                                                                                           </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">---</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">## 小结</span><span style=\"background-color: #272822\">                                                                                                        </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">从上述论文可以看到，当前 **LLM 安全研究** 主要聚焦在：</span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 如何**攻击**：越狱、提示注入、知识库投毒、membership inference、密码猜测等；</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 如何**防御**：内部概念控制（JBShield）、自我防御（SelfDefend）、提示混淆、防 RAG </span><span style=\"background-color: #272822\">                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">投毒、面向代码生成的安全知识注入等；</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 如何**评估**：任务级别的越狱评测平台、多模态 membership inference 基准等。</span><span style=\"background-color: #272822\">                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">如果你能具体说明更感兴趣的子方向（比如「RAG </span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">安全」「越狱攻防」「隐私泄露」「代码生成安全」等），我可以基于这些方向进一步帮你：</span><span style=\"background-color: #272822\">                             </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 归纳更多代表性论文，</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 整理研究脉络（早期工作 → 近期进展），</span><span style=\"background-color: #272822\">                                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">- 或者帮你设计一个小型的调研/开题框架。</span><span style=\"background-color: #272822\">                                                                        </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"\"\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                           </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m## LLM 安全相关代表性论文一览（基于 2025 年顶会检索）\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m根据数据库检索结果，以下是近年在 USENIX Security、CCS \u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m等顶级安全会议上发表的、与大模型（LLM）安全密切相关的论文。为了方便理解，我按**攻击 / \u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m防御类型**进行简单归类，并用通俗中文做简要说明。\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 一、Jailbreak（越狱）攻击与防御\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m这些工作主要研究：如何绕过 LLM 的安全对齐机制（越狱），以及如何抵御此类攻击。\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 1. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mManipulation  \u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：现有 LLM 易被「越狱提示词」诱导输出违规内容（暴力、违法、隐私等）。\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- \u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m**核心思路**：通过分析模型内部被激活的“概念”表示，识别出与越狱/有害内容相关的激活模式，然后对这些激活进行**抑制\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m或重写**，从而在内部层面阻断越狱效果，而不仅仅依赖提示词规则过滤。\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 安全防护不应只停留在「输入输出层」黑盒过滤，\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 利用可解释性/概念激活分析，可以在**模型内部**构建更稳健的安全防线。\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 2. Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：现有越狱研究多集中在具体“提示技巧”，缺乏从**任务层面**系统评估不同场景下 LLM 的安全脆弱性。\u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 构建一个**自动化的 jailbreaking 攻击与防御评测平台**，\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 从不同任务（如代码生成、问答、对话、翻译等）层面去挖掘 LLM 的漏洞，\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 系统比较各种攻击和防御机制在不同任务上的表现。\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - LLM 安全评估要考虑「**任务上下文**」，有的任务天生风险更大（如代码执行、系统操作等）。\u001b[0m\u001b[48;2;39;40;34m                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 3. SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner  \u001b[0m\u001b[48;2;39;40;34m                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：如何在不大幅修改模型架构的前提下，让 LLM 在真实系统中更好地抵御越狱？\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 利用 LLM 本身的推理与自我反思能力，让模型**自己检测输入是否包含越狱企图**，\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 在检测到风险时，自动调整响应策略（拒绝、重写、请求澄清等），\u001b[0m\u001b[48;2;39;40;34m                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 更偏向实用工程视角，可部署在现有系统中。\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 「用 LLM 守 LLM」是一个重要趋势，\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 通过链式思维、自我批评、多轮审查等机制，能在工程上显著增强安全性。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 4. TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts  \u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：即便模型经过安全对齐，仍可通过巧妙设计提示组合进行绕过。\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提出 **“Twin Prompts”（双提示）** \u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m概念：利用两个协同的提示，一个负责伪装/引导，另一个负责触发具体的有害行为，\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 系统研究这类双提示结构如何破坏对齐策略，并评估各类防御机制的效果。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 实际攻击常不是“单条提示”那么简单，而是复杂的对话/多提示结构，\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 防御设计时也要考虑**会话级别、多轮、多提示**的组合攻击。\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 5. Prompt Obfuscation for Large Language Models  \u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：在开放环境中部署 LLM 时，提示词（尤其是系统提示 / 指令）可能泄露敏感策略或被篡改。\u001b[0m\u001b[48;2;39;40;34m                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 研究如何对提示进行**混淆（obfuscation）**，既让模型仍然理解并执行指令，又让攻击者难以直接读取或逆向出策略，\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 在提示保护、prompt leak 防护等方向上给出系统性方法。\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 除了“内容安全”，还要关注**提示本身的机密性与完整性**（prompt confidentiality & integrity）。\u001b[0m\u001b[48;2;39;40;34m               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 二、隐私攻击与 Membership Inference\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 6. Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models  \u001b[0m\u001b[48;2;39;40;34m              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - Membership Inference Attack（MIA）试图判断某条数据是否被用来训练模型，\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 在很多实际场景中，攻击者只拿到“模型输出的标签/回答”，没有概率分布等详细信息。\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 提出针对 **“仅标签”输出（label-only）** 场景的 MIA 方法，\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 分析在预训练 LLM 上，这类攻击在何种设定下仍然可行，以及风险程度。\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 即使接口极度受限（只返回结果而非置信度），\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 训练数据隐私仍旧存在泄露风险，\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 对使用敏感数据（医疗、金融等）训练的大模型，要认真评估 MIA 风险。\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 7. Membership Inference Attacks Against Vision-Language Models  \u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：Vision-Language \u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mModels（VLMs）同时处理图像与文本，其训练数据可能包含大量敏感图像（人脸、隐私场景等）。\u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 系统性分析对 VLM 的 membership inference 攻击，\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 探究在多模态设定下，如何利用模型的图文输出特征判断某张图像/文本是否出现在训练集中。\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 多模态大模型的隐私攻击面更大，\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 需要针对图像、文本及其组合构建新的隐私保护机制（如 DP、多模态加噪等）。\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 三、数据与知识投毒（Poisoning）\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 8. PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models  \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - RAG（Retrieval-Augmented Generation）系统依赖外部知识库，\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 如果知识库被恶意污染（加入错误、有害、带后门的信息），LLM 生成内容会被误导甚至执行后门行为。\u001b[0m\u001b[48;2;39;40;34m               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 系统研究 **“知识腐蚀攻击”**：在检索库中注入特定干扰内容，\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 分析攻击者如何通过少量恶意文档显著改变 RAG 输出，\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 并探讨潜在的检测与防御方法。\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - RAG 系统一旦上线，其**检索库安全**极其关键，\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 需要对知识库进行访问控制、数据源可信度评估、周期性审计与异常检测。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 四、代码生成与软件安全\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 9. Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection  \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：CCS 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：LLM + RAG 用于代码生成时，模型可能输出不安全的代码（存在漏洞、后门、错误配置）。\u001b[0m\u001b[48;2;39;40;34m                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 通过**注入安全知识**（knowledge injection），在 RAG 流程中显式检索与安全相关的最佳实践、规则、样例，\u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 让模型在生成代码时，参考这些“安全教材”，从而减少典型安全漏洞。\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 为 LLM「补安全课」非常重要，\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 通过工具链、RAG 知识库、静态分析等组合，可显著提升代码生成安全性。\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 五、其他安全相关方向\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m### 10. Password Guessing Using Large Language Models  \u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m*会议：USENIX Security 2025*\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **问题**：评估 LLM 在**密码猜测**与破解场景中的能力与风险。\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **核心思路**：\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 利用 LLM 的语言建模能力，生成高概率/有结构的密码候选，\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 研究其在实际攻击中的效用，与传统密码猜测工具（如基于规则、概率模型的工具）对比。\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- **启示**：\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - LLM 本身也可能被用作攻击工具（密码猜测、社工邮件撰写、钓鱼内容生成等），\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m  - 安全研究需要正视“攻防两用”的双刃剑属性。\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 六、如何继续查找 LLM 安全相关文献？\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m如果你想系统性深入 LLM 安全方向，可以考虑以下检索关键词与方向：\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **越狱与对齐**  \u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 关键词：`LLM jailbreak`, `jailbreak attack`, `alignment`, `safety alignment`, `adversarial prompts`, \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m`prompt injection`\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **隐私与数据安全**  \u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 关键词：`membership inference attack`, `training data extraction`, `privacy leakage`, `data \u001b[0m\u001b[48;2;39;40;34m              \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mreconstruction`, `LLM privacy`\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **数据/知识投毒与后门**  \u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 关键词：`data poisoning`, `backdoor attack`, `RAG poisoning`, `knowledge corruption`, `prompt poisoning`\u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **RAG 与应用层安全**  \u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 关键词：`retrieval-augmented generation security`, `RAG security`, `tool-augmented LLM security`, `agents \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34msecurity`\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m5. **多模态与代码安全**  \u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m   - 关键词：`vision-language model security`, `secure code generation`, `LLM for security`, `security-oriented\u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mRAG`\u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m---\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m## 小结\u001b[0m\u001b[48;2;39;40;34m                                                                                                        \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m从上述论文可以看到，当前 **LLM 安全研究** 主要聚焦在：\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 如何**攻击**：越狱、提示注入、知识库投毒、membership inference、密码猜测等；\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 如何**防御**：内部概念控制（JBShield）、自我防御（SelfDefend）、提示混淆、防 RAG \u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m投毒、面向代码生成的安全知识注入等；\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 如何**评估**：任务级别的越狱评测平台、多模态 membership inference 基准等。\u001b[0m\u001b[48;2;39;40;34m                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m如果你能具体说明更感兴趣的子方向（比如「RAG \u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m安全」「越狱攻防」「隐私泄露」「代码生成安全」等），我可以基于这些方向进一步帮你：\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 归纳更多代表性论文，\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 整理研究脉络（早期工作 → 近期进展），\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m- 或者帮你设计一个小型的调研/开题框架。\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: ## LLM 安全相关代表性论文一览（基于 2025 年顶会检索）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">根据数据库检索结果，以下是近年在 USENIX Security、CCS </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">等顶级安全会议上发表的、与大模型（LLM）安全密切相关的论文。为了方便理解，我按**攻击 / </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">防御类型**进行简单归类，并用通俗中文做简要说明。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 一、Jailbreak（越狱）攻击与防御</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">这些工作主要研究：如何绕过 LLM 的安全对齐机制（越狱），以及如何抵御此类攻击。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 1. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Manipulation  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：现有 LLM 易被「越狱提示词」诱导输出违规内容（暴力、违法、隐私等）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**核心思路**：通过分析模型内部被激活的“概念”表示，识别出与越狱/有害内容相关的激活模式，然后对这些激活进行**抑制或重</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">写**，从而在内部层面阻断越狱效果，而不仅仅依赖提示词规则过滤。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 安全防护不应只停留在「输入输出层」黑盒过滤，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 利用可解释性/概念激活分析，可以在**模型内部**构建更稳健的安全防线。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 2. Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：现有越狱研究多集中在具体“提示技巧”，缺乏从**任务层面**系统评估不同场景下 LLM 的安全脆弱性。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 构建一个**自动化的 jailbreaking 攻击与防御评测平台**，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 从不同任务（如代码生成、问答、对话、翻译等）层面去挖掘 LLM 的漏洞，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 系统比较各种攻击和防御机制在不同任务上的表现。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - LLM 安全评估要考虑「**任务上下文**」，有的任务天生风险更大（如代码执行、系统操作等）。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 3. SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：如何在不大幅修改模型架构的前提下，让 LLM 在真实系统中更好地抵御越狱？</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 利用 LLM 本身的推理与自我反思能力，让模型**自己检测输入是否包含越狱企图**，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 在检测到风险时，自动调整响应策略（拒绝、重写、请求澄清等），</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 更偏向实用工程视角，可部署在现有系统中。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 「用 LLM 守 LLM」是一个重要趋势，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 通过链式思维、自我批评、多轮审查等机制，能在工程上显著增强安全性。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 4. TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：即便模型经过安全对齐，仍可通过巧妙设计提示组合进行绕过。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提出 **“Twin Prompts”（双提示）** 概念：利用两个协同的提示，一个负责伪装/引导，另一个负责触发具体的有害行为，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 系统研究这类双提示结构如何破坏对齐策略，并评估各类防御机制的效果。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 实际攻击常不是“单条提示”那么简单，而是复杂的对话/多提示结构，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 防御设计时也要考虑**会话级别、多轮、多提示**的组合攻击。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 5. Prompt Obfuscation for Large Language Models  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：在开放环境中部署 LLM 时，提示词（尤其是系统提示 / 指令）可能泄露敏感策略或被篡改。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 研究如何对提示进行**混淆（obfuscation）**，既让模型仍然理解并执行指令，又让攻击者难以直接读取或逆向出策略，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 在提示保护、prompt leak 防护等方向上给出系统性方法。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 除了“内容安全”，还要关注**提示本身的机密性与完整性**（prompt confidentiality &amp; integrity）。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 二、隐私攻击与 Membership Inference</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 6. Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - Membership Inference Attack（MIA）试图判断某条数据是否被用来训练模型，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 在很多实际场景中，攻击者只拿到“模型输出的标签/回答”，没有概率分布等详细信息。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 提出针对 **“仅标签”输出（label-only）** 场景的 MIA 方法，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 分析在预训练 LLM 上，这类攻击在何种设定下仍然可行，以及风险程度。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 即使接口极度受限（只返回结果而非置信度），</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 训练数据隐私仍旧存在泄露风险，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 对使用敏感数据（医疗、金融等）训练的大模型，要认真评估 MIA 风险。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 7. Membership Inference Attacks Against Vision-Language Models  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：Vision-Language Models（VLMs）同时处理图像与文本，其训练数据可能包含大量敏感图像（人脸、隐私场景等）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 系统性分析对 VLM 的 membership inference 攻击，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 探究在多模态设定下，如何利用模型的图文输出特征判断某张图像/文本是否出现在训练集中。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 多模态大模型的隐私攻击面更大，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 需要针对图像、文本及其组合构建新的隐私保护机制（如 DP、多模态加噪等）。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 三、数据与知识投毒（Poisoning）</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 8. PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - RAG（Retrieval-Augmented Generation）系统依赖外部知识库，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 如果知识库被恶意污染（加入错误、有害、带后门的信息），LLM 生成内容会被误导甚至执行后门行为。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 系统研究 **“知识腐蚀攻击”**：在检索库中注入特定干扰内容，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 分析攻击者如何通过少量恶意文档显著改变 RAG 输出，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 并探讨潜在的检测与防御方法。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - RAG 系统一旦上线，其**检索库安全**极其关键，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 需要对知识库进行访问控制、数据源可信度评估、周期性审计与异常检测。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 四、代码生成与软件安全</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 9. Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：CCS 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：LLM + RAG 用于代码生成时，模型可能输出不安全的代码（存在漏洞、后门、错误配置）。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 通过**注入安全知识**（knowledge injection），在 RAG 流程中显式检索与安全相关的最佳实践、规则、样例，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 让模型在生成代码时，参考这些“安全教材”，从而减少典型安全漏洞。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 为 LLM「补安全课」非常重要，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 通过工具链、RAG 知识库、静态分析等组合，可显著提升代码生成安全性。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 五、其他安全相关方向</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 10. Password Guessing Using Large Language Models  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">*会议：USENIX Security 2025*</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **问题**：评估 LLM 在**密码猜测**与破解场景中的能力与风险。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **核心思路**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 利用 LLM 的语言建模能力，生成高概率/有结构的密码候选，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 研究其在实际攻击中的效用，与传统密码猜测工具（如基于规则、概率模型的工具）对比。</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- **启示**：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - LLM 本身也可能被用作攻击工具（密码猜测、社工邮件撰写、钓鱼内容生成等），</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">  - 安全研究需要正视“攻防两用”的双刃剑属性。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 六、如何继续查找 LLM 安全相关文献？</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">如果你想系统性深入 LLM 安全方向，可以考虑以下检索关键词与方向：</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **越狱与对齐**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 关键词：`LLM jailbreak`, `jailbreak attack`, `alignment`, `safety alignment`, `adversarial prompts`, `prompt </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">injection`</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **隐私与数据安全**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 关键词：`membership inference attack`, `training data extraction`, `privacy leakage`, `data reconstruction`, </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">`LLM privacy`</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **数据/知识投毒与后门**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 关键词：`data poisoning`, `backdoor attack`, `RAG poisoning`, `knowledge corruption`, `prompt poisoning`</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **RAG 与应用层安全**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 关键词：`retrieval-augmented generation security`, `RAG security`, `tool-augmented LLM security`, `agents </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">security`</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">5. **多模态与代码安全**  </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">   - 关键词：`vision-language model security`, `secure code generation`, `LLM for security`, `security-oriented </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">RAG`</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">---</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">## 小结</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">从上述论文可以看到，当前 **LLM 安全研究** 主要聚焦在：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 如何**攻击**：越狱、提示注入、知识库投毒、membership inference、密码猜测等；</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 如何**防御**：内部概念控制（JBShield）、自我防御（SelfDefend）、提示混淆、防 RAG </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">投毒、面向代码生成的安全知识注入等；</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 如何**评估**：任务级别的越狱评测平台、多模态 membership inference 基准等。</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">如果你能具体说明更感兴趣的子方向（比如「RAG </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">安全」「越狱攻防」「隐私泄露」「代码生成安全」等），我可以基于这些方向进一步帮你：</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 归纳更多代表性论文，</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 整理研究脉络（早期工作 → 近期进展），</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- 或者帮你设计一个小型的调研/开题框架。</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: ## LLM 安全相关代表性论文一览（基于 2025 年顶会检索）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m根据数据库检索结果，以下是近年在 USENIX Security、CCS \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m等顶级安全会议上发表的、与大模型（LLM）安全密切相关的论文。为了方便理解，我按**攻击 / \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m防御类型**进行简单归类，并用通俗中文做简要说明。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 一、Jailbreak（越狱）攻击与防御\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m这些工作主要研究：如何绕过 LLM 的安全对齐机制（越狱），以及如何抵御此类攻击。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 1. JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mManipulation  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：现有 LLM 易被「越狱提示词」诱导输出违规内容（暴力、违法、隐私等）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m**核心思路**：通过分析模型内部被激活的“概念”表示，识别出与越狱/有害内容相关的激活模式，然后对这些激活进行**抑制或重\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m写**，从而在内部层面阻断越狱效果，而不仅仅依赖提示词规则过滤。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 安全防护不应只停留在「输入输出层」黑盒过滤，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 利用可解释性/概念激活分析，可以在**模型内部**构建更稳健的安全防线。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 2. Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：现有越狱研究多集中在具体“提示技巧”，缺乏从**任务层面**系统评估不同场景下 LLM 的安全脆弱性。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 构建一个**自动化的 jailbreaking 攻击与防御评测平台**，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 从不同任务（如代码生成、问答、对话、翻译等）层面去挖掘 LLM 的漏洞，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 系统比较各种攻击和防御机制在不同任务上的表现。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - LLM 安全评估要考虑「**任务上下文**」，有的任务天生风险更大（如代码执行、系统操作等）。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 3. SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：如何在不大幅修改模型架构的前提下，让 LLM 在真实系统中更好地抵御越狱？\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 利用 LLM 本身的推理与自我反思能力，让模型**自己检测输入是否包含越狱企图**，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 在检测到风险时，自动调整响应策略（拒绝、重写、请求澄清等），\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 更偏向实用工程视角，可部署在现有系统中。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 「用 LLM 守 LLM」是一个重要趋势，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 通过链式思维、自我批评、多轮审查等机制，能在工程上显著增强安全性。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 4. TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：即便模型经过安全对齐，仍可通过巧妙设计提示组合进行绕过。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提出 **“Twin Prompts”（双提示）** 概念：利用两个协同的提示，一个负责伪装/引导，另一个负责触发具体的有害行为，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 系统研究这类双提示结构如何破坏对齐策略，并评估各类防御机制的效果。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 实际攻击常不是“单条提示”那么简单，而是复杂的对话/多提示结构，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 防御设计时也要考虑**会话级别、多轮、多提示**的组合攻击。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 5. Prompt Obfuscation for Large Language Models  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：在开放环境中部署 LLM 时，提示词（尤其是系统提示 / 指令）可能泄露敏感策略或被篡改。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 研究如何对提示进行**混淆（obfuscation）**，既让模型仍然理解并执行指令，又让攻击者难以直接读取或逆向出策略，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 在提示保护、prompt leak 防护等方向上给出系统性方法。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 除了“内容安全”，还要关注**提示本身的机密性与完整性**（prompt confidentiality & integrity）。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 二、隐私攻击与 Membership Inference\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 6. Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - Membership Inference Attack（MIA）试图判断某条数据是否被用来训练模型，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 在很多实际场景中，攻击者只拿到“模型输出的标签/回答”，没有概率分布等详细信息。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 提出针对 **“仅标签”输出（label-only）** 场景的 MIA 方法，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 分析在预训练 LLM 上，这类攻击在何种设定下仍然可行，以及风险程度。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 即使接口极度受限（只返回结果而非置信度），\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 训练数据隐私仍旧存在泄露风险，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 对使用敏感数据（医疗、金融等）训练的大模型，要认真评估 MIA 风险。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 7. Membership Inference Attacks Against Vision-Language Models  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：Vision-Language Models（VLMs）同时处理图像与文本，其训练数据可能包含大量敏感图像（人脸、隐私场景等）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 系统性分析对 VLM 的 membership inference 攻击，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 探究在多模态设定下，如何利用模型的图文输出特征判断某张图像/文本是否出现在训练集中。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 多模态大模型的隐私攻击面更大，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 需要针对图像、文本及其组合构建新的隐私保护机制（如 DP、多模态加噪等）。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 三、数据与知识投毒（Poisoning）\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 8. PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - RAG（Retrieval-Augmented Generation）系统依赖外部知识库，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 如果知识库被恶意污染（加入错误、有害、带后门的信息），LLM 生成内容会被误导甚至执行后门行为。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 系统研究 **“知识腐蚀攻击”**：在检索库中注入特定干扰内容，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 分析攻击者如何通过少量恶意文档显著改变 RAG 输出，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 并探讨潜在的检测与防御方法。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - RAG 系统一旦上线，其**检索库安全**极其关键，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 需要对知识库进行访问控制、数据源可信度评估、周期性审计与异常检测。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 四、代码生成与软件安全\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 9. Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：CCS 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：LLM + RAG 用于代码生成时，模型可能输出不安全的代码（存在漏洞、后门、错误配置）。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 通过**注入安全知识**（knowledge injection），在 RAG 流程中显式检索与安全相关的最佳实践、规则、样例，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 让模型在生成代码时，参考这些“安全教材”，从而减少典型安全漏洞。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 为 LLM「补安全课」非常重要，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 通过工具链、RAG 知识库、静态分析等组合，可显著提升代码生成安全性。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 五、其他安全相关方向\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 10. Password Guessing Using Large Language Models  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m*会议：USENIX Security 2025*\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **问题**：评估 LLM 在**密码猜测**与破解场景中的能力与风险。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **核心思路**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 利用 LLM 的语言建模能力，生成高概率/有结构的密码候选，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 研究其在实际攻击中的效用，与传统密码猜测工具（如基于规则、概率模型的工具）对比。\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- **启示**：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - LLM 本身也可能被用作攻击工具（密码猜测、社工邮件撰写、钓鱼内容生成等），\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m  - 安全研究需要正视“攻防两用”的双刃剑属性。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 六、如何继续查找 LLM 安全相关文献？\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m如果你想系统性深入 LLM 安全方向，可以考虑以下检索关键词与方向：\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. **越狱与对齐**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 关键词：`LLM jailbreak`, `jailbreak attack`, `alignment`, `safety alignment`, `adversarial prompts`, `prompt \u001b[0m\n",
       "\u001b[1;38;2;212;183;2minjection`\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2. **隐私与数据安全**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 关键词：`membership inference attack`, `training data extraction`, `privacy leakage`, `data reconstruction`, \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m`LLM privacy`\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m3. **数据/知识投毒与后门**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 关键词：`data poisoning`, `backdoor attack`, `RAG poisoning`, `knowledge corruption`, `prompt poisoning`\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m4. **RAG 与应用层安全**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 关键词：`retrieval-augmented generation security`, `RAG security`, `tool-augmented LLM security`, `agents \u001b[0m\n",
       "\u001b[1;38;2;212;183;2msecurity`\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m5. **多模态与代码安全**  \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m   - 关键词：`vision-language model security`, `secure code generation`, `LLM for security`, `security-oriented \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mRAG`\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m---\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m## 小结\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m从上述论文可以看到，当前 **LLM 安全研究** 主要聚焦在：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 如何**攻击**：越狱、提示注入、知识库投毒、membership inference、密码猜测等；\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 如何**防御**：内部概念控制（JBShield）、自我防御（SelfDefend）、提示混淆、防 RAG \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m投毒、面向代码生成的安全知识注入等；\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 如何**评估**：任务级别的越狱评测平台、多模态 membership inference 基准等。\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m如果你能具体说明更感兴趣的子方向（比如「RAG \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m安全」「越狱攻防」「隐私泄露」「代码生成安全」等），我可以基于这些方向进一步帮你：\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 归纳更多代表性论文，\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 整理研究脉络（早期工作 → 近期进展），\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- 或者帮你设计一个小型的调研/开题框架。\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 82.06 seconds| Input tokens: 6,974 | Output tokens: 3,355]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 82.06 seconds| Input tokens: 6,974 | Output tokens: 3,355]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from smolagents import CodeAgent, OpenAIServerModel, ToolCollection\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a helpful research assistant.\n",
    "When given a question, you must query the database to get relevant information.\n",
    "Use the tools with appropriate arguments derived from the question.\n",
    "After getting the information, provide a comprehensive answer based on both the retrieved information.\n",
    "* Output the final answer in markdown wrapped in final_answer().\n",
    "\"\"\".strip()\n",
    "\n",
    "with ToolCollection.from_mcp(\n",
    "    {\"url\": \"http://127.0.0.1:8000/mcp\", \"transport\": \"streamable-http\"},\n",
    "    trust_remote_code=True,\n",
    "    structured_output=False,\n",
    ") as tool_collection:\n",
    "    agent = CodeAgent(\n",
    "        model=OpenAIServerModel(\"gpt-5.1\"),\n",
    "        tools=[*tool_collection.tools],\n",
    "        stream_outputs=True,\n",
    "        use_structured_outputs_internally=True,\n",
    "    )\n",
    "\n",
    "    # from Gradio_UI import GradioUI\n",
    "\n",
    "    # GradioUI(agent).launch()\n",
    "\n",
    "    agent.run(f\"{sys_prompt}\\n\\n{input('=> ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98876349",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- 数据基座：从 dblp 拉取信安四大会 2025 TOC，用 `OpenAIEmbeddings` 构建并持久化 `FAISS` 向量库，后续各方案共享。\n",
    "\n",
    "- 方案0（原生 tool call）：直接用 OpenAI `responses.create` + JSON Schema 工具。链路最短、调试透明、可强制 `tool_choice`；但对话状态管理、重试、去重、输出都要手写。\n",
    "\n",
    "- 方案1（SmolAgent::CodeAgent）：封装规划+多步推理，`use_structured_outputs_internally` 决定输出格式（叙事式 vs JSON）。自带 `@tool` 装饰、流式打印，适合想要自动规划、教学演示或快速接 UI 的场景。\n",
    "\n",
    "- 方案2（LangGraph）：用显式状态机把 LLM 节点与 `ToolNode` 串起来，`tools_condition` 控制循环；便于插入过滤/重排/裁剪等治理节点，可视化和监控更友好，适合可编排性与合规要求高的流水线。\n",
    "\n",
    "- 方案3（CodeAgent + MCP）：工具通过 MCP 服务集中托管，前端代理几乎零改动即可复用跨项目/多语言工具；适合团队内统一工具治理、远程扩展或把工具独立部署成服务。\n",
    "\n",
    "- 选型指南：\n",
    "\n",
    "  - 快速内嵌或最小依赖：选方案0。\n",
    "\n",
    "  - 需要自动规划、流式展示或教学：选方案1。\n",
    "\n",
    "  - 流程复杂、需可视化/治理节点：选方案2。\n",
    "\n",
    "  - 工具要集中化、跨项目共享或远程调用：选方案3。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
